{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55366\\AppData\\Local\\Temp\\ipykernel_9284\\2240545957.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(backgrounds)\n"
     ]
    }
   ],
   "source": [
    "def get_backgrounds():\n",
    "    backgrounds = []\n",
    "    for file in os.listdir(\"./images/train\"):\n",
    "        if file.endswith('.jpg'):\n",
    "            backgrounds.append(plt.imread(os.path.join(\"./images/train\",file)))\n",
    "    return np.array(backgrounds)\n",
    "backgrounds = get_backgrounds()\n",
    "\n",
    "\n",
    "def compose_image(image):\n",
    "    image = (image > 0).astype(np.float32)\n",
    "    image = image.reshape([28,28])*255.0\n",
    "    \n",
    "    image = np.stack([image,image,image],axis=2)\n",
    "    \n",
    "    background = np.random.choice(backgrounds)\n",
    "    w,h,_ = background.shape\n",
    "    dw, dh,_ = image.shape\n",
    "    x = np.random.randint(0,w-dw)\n",
    "    y = np.random.randint(0,h-dh)\n",
    "    \n",
    "    temp = background[x:x+dw, y:y+dh]\n",
    "    return np.abs(temp-image).astype(np.uint8)\n",
    "\n",
    "\n",
    "class MNISTM(Dataset):\n",
    "            \n",
    "    def __init__(self, train=True,transform=None):\n",
    "        if train:\n",
    "            self.data = datasets.MNIST(root='.data/mnist',train=True, download=True)\n",
    "        else:\n",
    "            self.data = datasets.MNIST(root='.data/mnist',train=False, download=True)\n",
    "        self.backgrounds = get_backgrounds()\n",
    "        self.transform = transform\n",
    "    def __getitem__(self,index):\n",
    "        image = np.array(self.data.__getitem__(index)[0])\n",
    "        target = self.data.__getitem__(index)[1]\n",
    "        image = compose_image(image)\n",
    "        #image = Image.fromarray(image.squeeze(), mode=\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "def get_mnistm_loaders(data_aug = False, batch_size=128,test_batch_size=1000):\n",
    "    if data_aug:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(64),\n",
    "            transforms.RandomCrop(64,padding=4),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    kwargs = {}\n",
    "    train_loader = DataLoader(\n",
    "        MNISTM(train=True,transform=train_transform),batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    train_eval_loader = DataLoader(\n",
    "        MNISTM(train=True, transform=test_transform),batch_size=test_batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader = DataLoader(\n",
    "        MNISTM(train=False,transform=test_transform),batch_size=test_batch_size, shuffle=False, drop_last=True)\n",
    "    return train_loader, train_eval_loader, test_loader\n",
    "\n",
    "\n",
    "def get_mnist_loaders(data_aug = False, batch_size=128,test_batch_size=1000):\n",
    "    if data_aug:\n",
    "        train_transform = transforms.Compose(\n",
    "            [transforms.Resize(64),\n",
    "            transforms.RandomCrop(64,padding=4),\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(64),\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    kwargs = {}\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(root='.data/mnist',train=True, download=True,transform=train_transform),batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    train_eval_loader = DataLoader(\n",
    "        datasets.MNIST(root='.data/mnist',train=True, download=True, transform=test_transform),batch_size=test_batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(root='.data/mnist',train=False, download=True, transform=test_transform),batch_size=test_batch_size, shuffle=False, drop_last=True)\n",
    "    return train_loader, train_eval_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55366\\AppData\\Local\\Temp\\ipykernel_9284\\2240545957.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(backgrounds)\n"
     ]
    }
   ],
   "source": [
    "loader_source, mnist_eval_loader, mnist_test_loader = get_mnist_loaders(batch_size=128)\n",
    "loader_target, mnistm_eval_loader,mnistm_test_loader = get_mnistm_loaders(batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_nc=3):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Initial convolution block\n",
    "        model = [   nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(input_nc, 64, 7),\n",
    "                    nn.InstanceNorm2d(64),\n",
    "                    nn.ReLU(inplace=True) ]\n",
    "\n",
    "        model += [nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "                  nn.InstanceNorm2d(128),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "                  \n",
    "        model += [nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "                  nn.InstanceNorm2d(256),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        model += [nn.Conv2d(256, 256, 3, stride=2, padding=1),\n",
    "                  nn.InstanceNorm2d(256),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        model += [nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "                  nn.InstanceNorm2d(512),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        model += [nn.Conv2d(512, 1024, 3, stride=2, padding=1),\n",
    "                  nn.InstanceNorm2d(1024),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_nc=1024, output_nc=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        model = [nn.ConvTranspose2d(input_nc, 512, 3, stride=2, padding=1, output_padding=1),\n",
    "                  nn.InstanceNorm2d(512),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        model += [nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1),\n",
    "                  nn.InstanceNorm2d(256),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "        \n",
    "        model += [nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "                  nn.InstanceNorm2d(128),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        model += [nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "                  nn.InstanceNorm2d(64),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        model += [nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "                  nn.InstanceNorm2d(64),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "                  \n",
    "        \n",
    "        # Output layer\n",
    "        model += [  nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(64, output_nc, 7),\n",
    "                    nn.InstanceNorm2d(3),\n",
    "                    nn.Tanh() ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Identity_Generator(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Identity_Generator, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        latentA = self.encoder(A)\n",
    "        latentB = self.encoder(B)\n",
    "\n",
    "        reconstructedA = self.decoder(latentA)\n",
    "        reconstructedB = self.decoder(latentB)\n",
    "        return reconstructedA, reconstructedB\n",
    "\n",
    "class Perceptual(nn.Module):\n",
    "    def __init__(self, encoder, decoder, generator):\n",
    "        super(Perceptual, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, A, B):\n",
    "\n",
    "        reconstructedA, reconstructedB = self.generator(A, B)\n",
    "\n",
    "        latentA = self.encoder(A)\n",
    "        latentB = self.encoder(B)\n",
    "\n",
    "        latentA.detach()\n",
    "        latentB.detach()\n",
    "\n",
    "        style = latentA[:, 0:512, : , :]\n",
    "        content = latentB[:, 512:1024, :, :]\n",
    "        \n",
    "        mixed_latent = torch.cat([style, content], dim=1)\n",
    "        mixed_image = self.decoder(mixed_latent)\n",
    "\n",
    "        return mixed_image, reconstructedA, reconstructedB\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc = 3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(64, 128, 4, padding=1),\n",
    "                    nn.InstanceNorm2d(128), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(256), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(256), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(512, 1, 4, padding=1)]\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  self.model(x)\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load S3gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptual = torch.load(\"./model_weight/perceptual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixed_image(perceptual, source, target, use_cuda):\n",
    "    mixed = []\n",
    "    label = []\n",
    "    reconstructed_A = []\n",
    "\n",
    "    data_target_iter = iter(target)\n",
    "    for _, (source_inputs, source_label) in enumerate(source):\n",
    "        batch_size = source_inputs.size(0)\n",
    "        target_inputs, target_label = data_target_iter.next()\n",
    "\n",
    "        if use_cuda: \n",
    "            source_inputs, target_inputs = source_inputs.cuda(), target_inputs.cuda()\n",
    "\n",
    "        mixed_image, reconstructed_source, _ = perceptual(source_inputs, target_inputs)\n",
    "\n",
    "        label.extend(source_label.detach().cpu().numpy())\n",
    "        mixed.extend(mixed_image.detach().cpu().numpy())\n",
    "        reconstructed_A.extend(reconstructed_source.detach().cpu().numpy())\n",
    "    \n",
    "    return mixed, reconstructed_A, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(\"./dataset/mixed.pt\")) == False and os.path.exists(\"./dataset/recon_A.pt\") == False:\n",
    "    mixed, reconstructionA, label = generate_mixed_image(perceptual, loader_target, loader_source, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(\"./dataset/mixed.pt\")) == False:\n",
    "    tensor_mixed = torch.Tensor(mixed)\n",
    "    tensor_label = torch.Tensor(label)\n",
    "\n",
    "    mixed_dataset = TensorDataset(tensor_mixed, tensor_label)\n",
    "    torch.save(mixed_dataset, './dataset/mixed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(\"./dataset/recon_A.pt\")) == False:\n",
    "    tensor_reconstructionA = torch.Tensor(reconstructionA)\n",
    "    tensor_label = torch.Tensor(label)\n",
    "\n",
    "    recon_dataset = TensorDataset(tensor_reconstructionA, tensor_label)\n",
    "    torch.save(recon_dataset, './dataset/recon_A.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_dataset = torch.load('./dataset/mixed.pt', map_location=torch.device('cpu'))\n",
    "mixed_dataloader = DataLoader(mixed_dataset, batch_size= 128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_dataset = torch.load('./dataset/recon_A.pt', map_location=torch.device('cpu'))\n",
    "recon_dataloader = DataLoader(mixed_dataset, batch_size= 128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(20, 3, 64, 64)\n",
    "out = F.interpolate(t, size=32)\n",
    "out.size()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4e5af365abde9565455761093e8e9bc7189c8b509b33061fd0f90eaa1b4d4cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
