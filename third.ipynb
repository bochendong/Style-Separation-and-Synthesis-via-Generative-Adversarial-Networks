{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision import transforms, models, datasets\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from pylab import imread\n",
        "\n",
        "import functools\n",
        "from functools import partial"
      ],
      "outputs": [],
      "metadata": {
        "id": "PIDNEoMIoA7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "if (os.path.exists(\"./model_weight\")) == False:\n",
        "    os.mkdir(\"model_weight\")\n",
        "\n",
        "for epoch in range (200):\n",
        "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "        os.mkdir(\"./output/%03d\" % epoch)\n",
        "    else:\n",
        "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "        for f in files:\n",
        "          os.remove(f)"
      ],
      "outputs": [],
      "metadata": {
        "id": "GjBJCEJeoA7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "kaErkB1roA7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True}\n",
        "\n",
        "cuda = True\n",
        "image_size = 32\n",
        "batchSize = 64"
      ],
      "outputs": [],
      "metadata": {
        "id": "jFqUAOXUoA73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "def show_img(source, target, source_label, target_label):\n",
        "    num_row = 4\n",
        "    num_col = 5\n",
        "    num = 10\n",
        "    images = source[:num]\n",
        "    labels = source_label[:num]\n",
        "\n",
        "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
        "    for i in range(num):\n",
        "        ax = axes[i//num_col, i%num_col]\n",
        "        \n",
        "        image =  images[i].transpose(0,2).transpose(0,1)\n",
        "\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title('Label: {}'.format(labels[i]))\n",
        "\n",
        "\n",
        "    images = target[:num]\n",
        "    labels = target_label[:num]\n",
        "    for i in range(10,20):\n",
        "        ax = axes[i//num_col, i%num_col]\n",
        "        image = images[i - 10].transpose(0,2).transpose(0,1)\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title('Label: {}'.format(labels[i - 10]))\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "-miReHhIoA75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.Grayscale(3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda t: t * 2 - 1)])\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='./data/mnist', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data/mnist', train=False, download=True, transform=transform)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "13as5mhOoA76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "def get_backgrounds():\n",
        "    backgrounds = []\n",
        "    for file in os.listdir(\"./images/train\"):\n",
        "        if file.endswith('.jpg'):\n",
        "            backgrounds.append(plt.imread(os.path.join(\"./images/train\",file)))\n",
        "    return backgrounds\n",
        "\n",
        "def compose_image(image, backgrounds):\n",
        "    image = (image > 0).astype(np.float32)\n",
        "    image = image.reshape([28,28])*255.0\n",
        "    \n",
        "    image = np.stack([image,image,image],axis=2)\n",
        "    \n",
        "    background = np.random.choice(backgrounds)\n",
        "    w,h,_ = background.shape\n",
        "    dw, dh,_ = image.shape\n",
        "    x = np.random.randint(0,w-dw)\n",
        "    y = np.random.randint(0,h-dh)\n",
        "    \n",
        "    temp = background[x:x+dw, y:y+dh]\n",
        "    return np.abs(temp-image).astype(np.uint8)\n",
        "\n",
        "class MNISTM(Dataset):    \n",
        "    def __init__(self, train=True,transform=None):\n",
        "        if train:\n",
        "            self.data = datasets.MNIST(root='.data/mnist',train=True, download=True)\n",
        "        else:\n",
        "            self.data = datasets.MNIST(root='.data/mnist',train=False, download=True)\n",
        "        self.backgrounds = get_backgrounds()\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.targets = []\n",
        "        for index in range(len(self.data)):\n",
        "            image = np.array(self.data.__getitem__(index)[0])\n",
        "            target = self.data.__getitem__(index)[1]\n",
        "            image = compose_image(image, self.backgrounds)\n",
        "            if self.transform is not None:\n",
        "                image = self.transform(image)\n",
        "            self.images.append(image)\n",
        "            self.targets.append(target)\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        image = self.images[index]\n",
        "        target = self.targets[index]\n",
        "        \n",
        "        return image, target\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "outputs": [],
      "metadata": {
        "id": "T9uUpScqoA77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: t * 2 - 1)\n",
        "        ])\n",
        "\n",
        "trainset = MNISTM(train=True,transform=transform)\n",
        "testset = MNISTM(train=False,transform=transform)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9N3L3UpoA79",
        "outputId": "7e895382-b860-4365-9fde-0600664eac8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "source_train = DataLoader(mnist_trainset, batch_size=batchSize, shuffle=True, drop_last=True, **kwargs)\n",
        "source_test = DataLoader(mnist_testset, batch_size=batchSize, shuffle=True, drop_last=True, **kwargs)\n",
        "\n",
        "target_train = DataLoader(trainset, batch_size=batchSize, shuffle=True, drop_last=True, **kwargs)\n",
        "target_test = DataLoader(testset, batch_size=batchSize, shuffle=False, drop_last=True, **kwargs)"
      ],
      "outputs": [],
      "metadata": {
        "id": "u3QsqRRIoA7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module"
      ],
      "metadata": {
        "id": "7BZ_W1dvoA7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_iter = iter(source_train)\n",
        "source_inputs, source_label = source_iter.next()\n",
        "\n",
        "target_iter = iter(target_train)\n",
        "target_inputs, target_label = target_iter.next()"
      ],
      "metadata": {
        "id": "UAP38dIJoHGq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, filters=64, kernel_size=3, stride=1, padding=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        bin = functools.partial(Normlayer, affine=True)\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, filters, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            bin(filters),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(filters, filters, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            bin(filters)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != filters:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, filters, kernel_size=1, stride=stride, bias=False),\n",
        "                bin(filters)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.main(x)\n",
        "        output += self.shortcut(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "OV4OpfGZEXXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, channels=3):\n",
        "        super(Encoder, self).__init__()\n",
        "        bin = functools.partial(Normlayer, affine=True)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(channels, 32, kernel_size=4, stride=2, padding=1, bias=True),\n",
        "            bin(32),\n",
        "            nn.ReLU(True),\n",
        "            ResidualBlock(32, 32),\n",
        "            ResidualBlock(32, 32),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "JW-opIj3Eckp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''class DRAN(nn.Module):\n",
        "    def __init__(self, residual=True):\n",
        "        super(DRAN, self).__init__()\n",
        "        self.residual = residual\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer_0 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.layer_1 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.layer_2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(32)\n",
        "        )\n",
        "        self.layer_2_downsample = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False),\n",
        "            nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "\n",
        "        self.layer_3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "\n",
        "        self.layer_3_downsample = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "\n",
        "        self.layer_4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "\n",
        "        self.layer_4_downsample = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "\n",
        "        self.layer_5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "\n",
        "        self.layer_5_downsample = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(256)\n",
        "        )\n",
        "\n",
        "        self.layer_6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "\n",
        "        self.layer_6_downsample = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512)\n",
        "        )\n",
        "\n",
        "        self.layer_7 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.layer_8 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=28, stride=28, padding=0)\n",
        "        self.fc = nn.Conv2d(512, 19, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        layer_0_out = self.layer_0(input)\n",
        "        layer_1_out = self.layer_1(layer_0_out)\n",
        "\n",
        "        layer_2_out = self.layer_2(layer_1_out)\n",
        "        if (self.residual):\n",
        "          layer_2_out += self.layer_2_downsample(layer_1_out)\n",
        "        layer_2_out = self.relu(layer_2_out)\n",
        "\n",
        "        layer_3_out = self.layer_3(layer_2_out)\n",
        "        if (self.residual):\n",
        "          layer_3_out += self.layer_3_downsample(layer_2_out)\n",
        "        layer_3_out = self.relu(layer_3_out)\n",
        "\n",
        "        layer_4_out = self.layer_4(layer_3_out)\n",
        "        if (self.residual):\n",
        "          layer_4_out += self.layer_4_downsample(layer_3_out)\n",
        "        layer_4_out = self.relu(layer_4_out)\n",
        "\n",
        "        layer_5_out = self.layer_5(layer_4_out)\n",
        "        if (self.residual):\n",
        "          layer_5_out += self.layer_5_downsample(layer_4_out)\n",
        "        layer_5_out = self.relu(layer_5_out)\n",
        "\n",
        "        layer_6_out = self.layer_6(layer_5_out)\n",
        "        if (self.residual):\n",
        "          layer_6_out += self.layer_6_downsample(layer_5_out)\n",
        "        layer_6_out = self.relu(layer_6_out)\n",
        "\n",
        "        layer_7_out = self.layer_7(layer_6_out)\n",
        "        layer_8_out = self.layer_8(layer_7_out)\n",
        "\n",
        "        out = self.avgpool(layer_8_out)\n",
        "        out = self.fc(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "'''"
      ],
      "metadata": {
        "id": "_u8_2ZB68m54"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dran = DRAN()"
      ],
      "metadata": {
        "id": "H25_ILyQ_0bL"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = dran(source_inputs)\n",
        "out.size()"
      ],
      "metadata": {
        "id": "N3LTghWV_-Ki",
        "outputId": "d2a0e023-b26c-422c-9c75-d9c500e1a29d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-3121d0b6ebe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdran\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-39bad39f6450>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mlayer_8_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_7_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_8_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m--> 623\u001b[0;31m                             self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (512x4x4). Calculated output size: (512x0x0). Output size is too small"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "third.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}