{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "tAz7koKv1MB5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision import transforms, models, datasets\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from pylab import imread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2i0ul1TdfDUu"
      },
      "outputs": [],
      "source": [
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "if (os.path.exists(\"./model_weight\")) == False:\n",
        "    os.mkdir(\"model_weight\")\n",
        "\n",
        "for epoch in range (200):\n",
        "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "        os.mkdir(\"./output/%03d\" % epoch)\n",
        "    else:\n",
        "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "        for f in files:\n",
        "          os.remove(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUJIfmDn1MCx"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True}\n",
        "\n",
        "cuda = True\n",
        "image_size = 32\n",
        "batchSize = 64"
      ],
      "metadata": {
        "id": "50aUu3MfiUvQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_img(source, target, source_label, target_label):\n",
        "    num_row = 4\n",
        "    num_col = 5\n",
        "    num = 10\n",
        "    images = source[:num]\n",
        "    labels = source_label[:num]\n",
        "\n",
        "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
        "    for i in range(num):\n",
        "        ax = axes[i//num_col, i%num_col]\n",
        "        \n",
        "        image =  images[i].transpose(0,2).transpose(0,1)\n",
        "\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title('Label: {}'.format(labels[i]))\n",
        "\n",
        "\n",
        "    images = target[:num]\n",
        "    labels = target_label[:num]\n",
        "    for i in range(10,20):\n",
        "        ax = axes[i//num_col, i%num_col]\n",
        "        image = images[i - 10].transpose(0,2).transpose(0,1)\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title('Label: {}'.format(labels[i - 10]))\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "osdNOtN2mpk0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mnist"
      ],
      "metadata": {
        "id": "kluPV1H5oETL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.Grayscale(3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda t: t * 2 - 1)])\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='./data/mnist', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data/mnist', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "OaqWTwLNn4Xa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mnist-M"
      ],
      "metadata": {
        "id": "QkU7PlPEoHUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_backgrounds():\n",
        "    backgrounds = []\n",
        "    for file in os.listdir(\"./images/train\"):\n",
        "        if file.endswith('.jpg'):\n",
        "            backgrounds.append(plt.imread(os.path.join(\"./images/train\",file)))\n",
        "    return backgrounds\n",
        "\n",
        "def compose_image(image, backgrounds):\n",
        "    image = (image > 0).astype(np.float32)\n",
        "    image = image.reshape([28,28])*255.0\n",
        "    \n",
        "    image = np.stack([image,image,image],axis=2)\n",
        "    \n",
        "    background = np.random.choice(backgrounds)\n",
        "    w,h,_ = background.shape\n",
        "    dw, dh,_ = image.shape\n",
        "    x = np.random.randint(0,w-dw)\n",
        "    y = np.random.randint(0,h-dh)\n",
        "    \n",
        "    temp = background[x:x+dw, y:y+dh]\n",
        "    return np.abs(temp-image).astype(np.uint8)\n",
        "\n",
        "class MNISTM(Dataset):    \n",
        "    def __init__(self, train=True,transform=None):\n",
        "        if train:\n",
        "            self.data = datasets.MNIST(root='.data/mnist',train=True, download=True)\n",
        "        else:\n",
        "            self.data = datasets.MNIST(root='.data/mnist',train=False, download=True)\n",
        "        self.backgrounds = get_backgrounds()\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.targets = []\n",
        "        for index in range(len(self.data)):\n",
        "            image = np.array(self.data.__getitem__(index)[0])\n",
        "            target = self.data.__getitem__(index)[1]\n",
        "            image = compose_image(image, self.backgrounds)\n",
        "            if self.transform is not None:\n",
        "                image = self.transform(image)\n",
        "            self.images.append(image)\n",
        "            self.targets.append(target)\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        image = self.images[index]\n",
        "        target = self.targets[index]\n",
        "        \n",
        "        return image, target\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "fNrMH4z5oGtu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: t * 2 - 1)\n",
        "        ])\n",
        "\n",
        "trainset = MNISTM(train=True,transform=transform)\n",
        "testset = MNISTM(train=False,transform=transform)"
      ],
      "metadata": {
        "id": "XXagwGNxov2t",
        "outputId": "1e79088a-37aa-49af-e0a7-2c2bb59e53c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_train = DataLoader(mnist_trainset, batch_size=batchSize, shuffle=True, drop_last=True, **kwargs)\n",
        "source_test = DataLoader(mnist_testset, batch_size=batchSize, shuffle=True, drop_last=True, **kwargs)"
      ],
      "metadata": {
        "id": "BRcXrUIZv6CF"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_train = DataLoader(trainset, batch_size=batchSize, shuffle=True, drop_last=True, **kwargs)\n",
        "target_test = DataLoader(testset, batch_size=batchSize, shuffle=False, drop_last=True, **kwargs)"
      ],
      "metadata": {
        "id": "-IVxtlhAoyRU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_iter = iter(source_train)\n",
        "source_inputs, source_label = source_iter.next()\n",
        "\n",
        "target_iter = iter(target_train)\n",
        "target_inputs, target_label = target_iter.next()"
      ],
      "metadata": {
        "id": "a9w67Lw0mrOY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%2f, %2f\" % (torch.min(source_inputs), torch.max(source_inputs)))"
      ],
      "metadata": {
        "id": "Ht_5WRfiryXC",
        "outputId": "555cf450-52d1-4926-d405-e109aed077be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.000000, 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%2f, %2f\" % (torch.min(target_inputs), torch.max(target_inputs)))"
      ],
      "metadata": {
        "id": "WtYBA8tBr-dX",
        "outputId": "0b6f8932-838c-41ac-be61-1fb5cb75119a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.000000, 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = random.randint(0, 15)\n",
        "image_source_sample = source_inputs[index]\n",
        "image_source_sample = (image_source_sample + 1) * 0.5\n",
        "image = image_source_sample.transpose(0,2).transpose(0,1)\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "ycvMXFa7cihR",
        "outputId": "d729d2d7-4fed-47cb-829d-4bc33eb38b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0ba1e7e410>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_target_sample = target_inputs[index]\n",
        "image_target_sample = (image_target_sample + 1) * 0.5\n",
        "image = image_target_sample.transpose(0,2).transpose(0,1)\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "RVS5SkcFdS3Q",
        "outputId": "ed118775-ce3e-4ed9-c335-d15fee64c6f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0ba7f03690>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj4IrxmY1MDH"
      },
      "source": [
        "# Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kV1qBY41MDO"
      },
      "source": [
        "## Perceptual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "NFsKAlmUdbsH"
      },
      "outputs": [],
      "source": [
        "class Perceptual(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Perceptual, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(3, 64, 7),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(64, 3, 7),\n",
        "            nn.InstanceNorm2d(3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, A, B):\n",
        "        encode_A = self.encoder(A)\n",
        "        encode_B = self.encoder(B)\n",
        "        reconA = self.decoder(encode_A)\n",
        "        reconB = self.decoder(encode_B)\n",
        "\n",
        "        encode_A.detach()\n",
        "        encode_B.detach()\n",
        "\n",
        "        style = encode_A[:, 0:128, : , :]\n",
        "        content = encode_B[:, 128:256, :, :]\n",
        "        \n",
        "        mixed_latent = torch.cat([style, content], dim=1)\n",
        "        mixed_image = self.decoder(mixed_latent)\n",
        "\n",
        "        return mixed_image, reconA, reconB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tensor_source = source_inputs\n",
        "test_tensor_target = target_inputs\n",
        "\n",
        "perceptual = Perceptual()\n",
        "encoder = perceptual.encoder\n",
        "decoder = perceptual.decoder\n",
        "encoder_out = encoder(test_tensor_target)\n",
        "decoder_out = decoder(encoder_out)\n",
        "\n",
        "print(encoder_out.size())\n",
        "print(decoder_out.size())\n",
        "\n",
        "mixed, reconA, reconB = perceptual(test_tensor_source, test_tensor_target)\n",
        "print(mixed.size())\n",
        "print(reconA.size())\n",
        "print(reconB.size())\n",
        "\n",
        "print(\"mixed: min: %.2f, max: %.2f \" % (torch.min(mixed), torch.max(mixed)))\n",
        "\n",
        "print(\"reconA: min: %.2f, max: %.2f \" % (torch.min(reconA), torch.max(reconA)))\n",
        "\n",
        "print(\"reconB: min: %.2f, max: %.2f \" % (torch.min(reconB), torch.max(reconB)))"
      ],
      "metadata": {
        "id": "ApdMclXUeRUC",
        "outputId": "cb55cd86-d3e7-4ae0-8f04-ff9bf79e94b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 256, 8, 8])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "mixed: min: -1.00, max: 1.00 \n",
            "reconA: min: -1.00, max: 1.00 \n",
            "reconB: min: -1.00, max: 1.00 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mixed[0][0][0] - reconA[0][0][0])\n",
        "print(mixed[0][0][0] - reconB[0][0][0])"
      ],
      "metadata": {
        "id": "M6NIU0nTgYJx",
        "outputId": "d68470c5-42e6-4c8c-d4be-fdbfc6e3ab45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.1954, -0.4117,  0.1019,  0.0136,  0.2273, -0.0589, -0.0590, -0.0170,\n",
            "         0.3245, -0.1014, -0.0675, -0.3079, -0.4429, -0.0862,  0.3751, -0.0834,\n",
            "         0.1536,  0.6722, -0.4725, -0.1030,  0.1380, -0.0765, -0.0038, -0.4069,\n",
            "         0.7215, -0.5400, -0.1986,  0.0018,  0.3890,  0.1243, -0.5683,  0.2041],\n",
            "       grad_fn=<SubBackward0>)\n",
            "tensor([-0.2169,  0.1935, -0.0521, -0.2751,  0.0223,  0.3839, -0.0408, -0.7108,\n",
            "         0.1007,  0.3237, -0.1082, -0.1871,  0.0491, -0.4472,  0.5514, -0.6972,\n",
            "        -0.8616,  0.9997, -0.5995,  0.2287,  0.6828,  0.9967, -0.7518, -0.5778,\n",
            "         0.6721, -0.2188, -0.1555, -0.2560, -0.1810, -0.1746, -0.1241, -0.0027],\n",
            "       grad_fn=<SubBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLubC4sJ1MDd"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "67DdNmWP1MDd"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc = 3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, padding=1),\n",
        "            nn.InstanceNorm2d(128), \n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, padding=1),\n",
        "            nn.InstanceNorm2d(256), \n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  self.model(x)\n",
        "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator()\n",
        "\n",
        "output = discriminator(test_tensor_source)\n",
        "print(output.size())\n",
        "print(torch.min(output))\n",
        "print(torch.max(output))"
      ],
      "metadata": {
        "id": "SY5fBnHMgAYz",
        "outputId": "51f97c70-d8ea-47c6-da71-b602b975eee8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1])\n",
            "tensor(-0.2517, grad_fn=<MinBackward1>)\n",
            "tensor(-0.1936, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPaeTfIX1MDk"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "6EkNLO_T1MDk"
      },
      "outputs": [],
      "source": [
        "def tv_loss(img, tv_weight=5e-2):\n",
        "    w_variance = torch.sum(torch.pow(img[:,:,:,:-1] - img[:,:,:,1:], 2))\n",
        "    h_variance = torch.sum(torch.pow(img[:,:,:-1,:] - img[:,:,1:,:], 2))\n",
        "    loss = tv_weight * (h_variance + w_variance)\n",
        "    return loss\n",
        "\n",
        "def total_variation_loss(img, weight=5e-2):\n",
        "    bs_img, c_img, h_img, w_img = img.size()\n",
        "    tv_h = torch.pow(img[:, :, 1:, :] - img[:, :, :-1, :], 2).sum()\n",
        "    tv_w = torch.pow(img[:, :, :, 1:] - img[:, :, :, :-1], 2).sum()\n",
        "    return weight * (tv_h + tv_w) / (bs_img * c_img * h_img * w_img)\n",
        "\n",
        "def compute_content_loss(target_feature, content_feature):\n",
        "    return torch.mean((target_feature - content_feature)**2)\n",
        "\n",
        "def batch_gram_matrix(img):\n",
        "    b, d, h, w = img.size()\n",
        "    img = img.view(b*d, h*w)\n",
        "    gram = torch.mm(img, img.t())\n",
        "    return gram\n",
        "    \n",
        "style_weights = {'conv1_1': 1.0, 'conv2_1': 0.75, 'conv3_1': 0.2, 'conv4_1': 0.2, 'conv5_1': 0.2}\n",
        "\n",
        "def compute_style_loss(style_features, target_features):\n",
        "    style_loss = 0\n",
        "    style_grams = {layer: batch_gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "    for layer in style_weights:\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = batch_gram_matrix(target_feature)\n",
        "        _, d, h, w = target_feature.shape\n",
        "\n",
        "        style_gram = style_grams[layer]\n",
        "\n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n",
        "        style_loss += layer_style_loss / (d * h * w)\n",
        "\n",
        "    return style_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEujnWnX1MDl"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4U1o05Y1MDm"
      },
      "source": [
        "## Training Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "_L7ExZCq1MDz"
      },
      "outputs": [],
      "source": [
        "def get_features(image, model, layers=None):\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1', \n",
        "              '3': 'conv2_1', \n",
        "              '6': 'conv3_1', \n",
        "              '11': 'conv4_1',\n",
        "              '13': 'conv4_2', \n",
        "              '16': 'conv5_1'}\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVvM_If_1MD1"
      },
      "source": [
        "## Model hypermeter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "TBdlE72j1MD4"
      },
      "outputs": [],
      "source": [
        "perceptual = Perceptual()\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Y8zIpzSL1MD5"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.05\n",
        "beta = (0.5, 0.999)\n",
        "\n",
        "criterion_adv = torch.nn.MSELoss()\n",
        "criterion_discriminator = torch.nn.MSELoss()\n",
        "criterion_construct = torch.nn.L1Loss()\n",
        "\n",
        "optimizer_pre = torch.optim.Adam(perceptual.parameters(), lr=learning_rate, betas=beta)\n",
        "optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=beta)\n",
        "\n",
        "scheduler_pre = StepLR(optimizer_pre, step_size=40, gamma=0.4)\n",
        "scheduler_dis = StepLR(optimizer_dis, step_size=40, gamma=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "OCYbEweS1MD5"
      },
      "outputs": [],
      "source": [
        "vgg = models.vgg11(pretrained=True).features\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "1_h8u5-V1MD5"
      },
      "outputs": [],
      "source": [
        "if (torch.cuda.is_available()):\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    criterion_construct = criterion_construct.cuda()\n",
        "    criterion_discriminator = criterion_discriminator.cuda()\n",
        "    criterion_construct = criterion_construct.cuda()\n",
        "\n",
        "    vgg.cuda()\n",
        "    perceptual.cuda()\n",
        "    discriminator.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe87ze5D1MD6"
      },
      "source": [
        "## Init the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vVGb29b_1MD6"
      },
      "outputs": [],
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant(m.bias.data, 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-cEljb71MD6",
        "outputId": "8f5e3254-56b2-4368-ff0e-242ad514c3bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "perceptual.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j-yi9aD1MD7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "real_label = Variable(Tensor(batchSize).fill_(1.0), requires_grad=False)\n",
        "fake_label = Variable(Tensor(batchSize).fill_(0.0), requires_grad=False)\n",
        "real_label = real_label[:, None]\n",
        "fake_label = fake_label[:, None]\n",
        "\n",
        "input_s = Tensor(batchSize, 3, image_size, image_size)\n",
        "input_t = Tensor(batchSize, 3, image_size, image_size)\n",
        "\n",
        "if cuda:\n",
        "  input_s, input_t = input_s.cuda(), input_t.cuda()"
      ],
      "metadata": {
        "id": "66yT_3H3i59X"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mixed image will have same label with source\n",
        "def training(source, target, perceptual, discriminator, \n",
        "             critic_adv, cirtic_recon, cirtic_dis, \n",
        "             optim_pre, optim_dis, \n",
        "             sche_pre, sche_dis, \n",
        "             use_cuda = True):\n",
        "    source_iter = iter(source)\n",
        "    target_iter = iter(target)\n",
        "\n",
        "    len_dataloader = min(len(source_iter), len(target_iter))\n",
        "\n",
        "    i = 0\n",
        "    while i < len_dataloader:\n",
        "        s_img, _ = source_iter.next()\n",
        "        t_img, _ = target_iter.next()\n",
        "\n",
        "        if use_cuda:\n",
        "            s_img, t_img = s_img.cuda(), t_img.cuda()\n",
        "\n",
        "        org_s = Variable(input_s.copy_(s_img))\n",
        "        org_t = Variable(input_t.copy_(t_img))\n",
        "\n",
        "        # Preceptual\n",
        "        optimizer_pre.zero_grad()\n",
        "        mixed, recon_s, recon_t = perceptual(org_s, org_t)\n",
        "        mixed_label = discriminator(mixed)\n",
        "\n",
        "        loss_ss = cirtic_recon(recon_s, org_s) * 5.0\n",
        "        loss_tt = cirtic_recon(recon_t, org_t) * 5.0\n",
        "\n",
        "        # if the discriminator predit false, loss_adv will decrease\n",
        "        loss_adv = critic_adv(mixed_label, real_label)\n",
        "\n",
        "        ## what if we calculate loss between (recon_s and recon_t)\n",
        "        ## what if we calculate loss between (recon_s and previous loss_ss) and (loss_tt and previous loss_tt)\n",
        "        ## what if we connect a classifier to the end of the perceptual\n",
        "        ## what if we calculate loss from previous epoch\n",
        "\n",
        "        TV_loss = total_variation_loss(mixed)\n",
        "\n",
        "        preceptual_loss = loss_ss + loss_tt + loss_adv + TV_loss\n",
        "        preceptual_loss.backward()\n",
        "        optim_pre.step()\n",
        "\n",
        "        # Discriminator\n",
        "        optim_dis.zero_grad()\n",
        "\n",
        "        pred_real = discriminator(org_s)\n",
        "        pred_fake = discriminator(mixed.detach())\n",
        "\n",
        "        loss_dis_real = cirtic_dis(pred_real, real_label)\n",
        "        loss_dis_fake = cirtic_dis(pred_fake, fake_label)\n",
        "\n",
        "        discriminator_loss = loss_dis_real + loss_dis_fake\n",
        "        discriminator_loss.backward()\n",
        "        optim_dis.step()\n",
        "\n",
        "        if  i % 400 == 0:\n",
        "            real_A = org_s.data\n",
        "            real_B = org_t.data\n",
        "            mixed_image = mixed.data\n",
        "            reconstructionA = recon_s.data\n",
        "            reconstructionB = recon_t.data\n",
        "\n",
        "            save_image(real_A, './output/%03d/%d_A.png' % ( epoch, i))\n",
        "            save_image(real_B, 'output/%03d/%d_B.png' % ( epoch, i))\n",
        "            save_image(reconstructionA, 'output/%03d/%d_reconA.png' % ( epoch, i))\n",
        "            save_image(reconstructionB, 'output/%03d/%d_reconB.png' % ( epoch, i))\n",
        "            save_image(mixed_image, 'output/%03d/%d_Mixed.png' % ( epoch, i))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    print (\"e: %d\" % epoch)    \n",
        "    print (\"pre_loss: %.2f, loss_ss: %.2f, loss_tt: %.2f, loss_adv: %.2f\" % (preceptual_loss, loss_ss, loss_tt, loss_adv))\n",
        "    print (\"discriminator_loss: %.2f, D_real: %.2f, D_fake: %.2f\" % (discriminator_loss, loss_dis_real, loss_dis_fake))\n",
        "\n",
        "    sche_pre.step()\n",
        "    sche_dis.step()\n",
        "      "
      ],
      "metadata": {
        "id": "sj9jKzg1i-50"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, 200):\n",
        "  training(source_train, target_train, perceptual, discriminator, \n",
        "           criterion_adv, criterion_construct, criterion_discriminator, \n",
        "           optimizer_pre, optimizer_dis, \n",
        "           scheduler_pre, scheduler_dis, use_cuda = True)"
      ],
      "metadata": {
        "id": "-5gjL3wwjQ1Z",
        "outputId": "c0dca69e-cbdc-4ed0-d3c6-204d2b8336cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 0\n",
            "pre_loss: 11.59, loss_ss: 4.05, loss_tt: 2.61, loss_adv: 4.92\n",
            "discriminator_loss: 5.50, D_real: 1.49, D_fake: 4.01\n",
            "e: 1\n",
            "pre_loss: 7.51, loss_ss: 3.99, loss_tt: 2.60, loss_adv: 0.89\n",
            "discriminator_loss: 0.32, D_real: 0.22, D_fake: 0.10\n",
            "e: 2\n",
            "pre_loss: 10.54, loss_ss: 3.77, loss_tt: 2.45, loss_adv: 4.30\n",
            "discriminator_loss: 2.69, D_real: 1.52, D_fake: 1.17\n",
            "e: 3\n",
            "pre_loss: 13.68, loss_ss: 3.47, loss_tt: 2.16, loss_adv: 8.03\n",
            "discriminator_loss: 6.63, D_real: 3.25, D_fake: 3.37\n",
            "e: 4\n",
            "pre_loss: 6.64, loss_ss: 3.29, loss_tt: 2.13, loss_adv: 1.21\n",
            "discriminator_loss: 0.06, D_real: 0.04, D_fake: 0.02\n",
            "e: 5\n",
            "pre_loss: 11.06, loss_ss: 4.24, loss_tt: 2.70, loss_adv: 4.09\n",
            "discriminator_loss: 5.48, D_real: 2.23, D_fake: 3.24\n",
            "e: 6\n",
            "pre_loss: 8.01, loss_ss: 4.49, loss_tt: 2.81, loss_adv: 0.67\n",
            "discriminator_loss: 1.44, D_real: 0.82, D_fake: 0.62\n",
            "e: 7\n",
            "pre_loss: 7.80, loss_ss: 4.40, loss_tt: 2.69, loss_adv: 0.66\n",
            "discriminator_loss: 0.61, D_real: 0.31, D_fake: 0.30\n",
            "e: 8\n",
            "pre_loss: 7.91, loss_ss: 4.40, loss_tt: 2.59, loss_adv: 0.87\n",
            "discriminator_loss: 0.23, D_real: 0.14, D_fake: 0.09\n",
            "e: 9\n",
            "pre_loss: 7.87, loss_ss: 3.62, loss_tt: 2.04, loss_adv: 2.18\n",
            "discriminator_loss: 0.88, D_real: 0.64, D_fake: 0.25\n",
            "e: 10\n",
            "pre_loss: 9.79, loss_ss: 4.51, loss_tt: 3.36, loss_adv: 1.79\n",
            "discriminator_loss: 3.64, D_real: 1.60, D_fake: 2.05\n",
            "e: 11\n",
            "pre_loss: 8.48, loss_ss: 4.23, loss_tt: 3.01, loss_adv: 1.12\n",
            "discriminator_loss: 0.29, D_real: 0.11, D_fake: 0.18\n",
            "e: 12\n",
            "pre_loss: 7.19, loss_ss: 3.64, loss_tt: 2.46, loss_adv: 1.04\n",
            "discriminator_loss: 0.25, D_real: 0.11, D_fake: 0.14\n",
            "e: 13\n",
            "pre_loss: 6.21, loss_ss: 3.49, loss_tt: 2.15, loss_adv: 0.54\n",
            "discriminator_loss: 0.17, D_real: 0.07, D_fake: 0.09\n",
            "e: 14\n",
            "pre_loss: 5.99, loss_ss: 3.37, loss_tt: 1.99, loss_adv: 0.61\n",
            "discriminator_loss: 7.01, D_real: 3.88, D_fake: 3.13\n",
            "e: 15\n",
            "pre_loss: 9.04, loss_ss: 4.52, loss_tt: 2.90, loss_adv: 1.52\n",
            "discriminator_loss: 0.91, D_real: 0.74, D_fake: 0.17\n",
            "e: 16\n",
            "pre_loss: 33.14, loss_ss: 3.60, loss_tt: 2.44, loss_adv: 27.06\n",
            "discriminator_loss: 35.64, D_real: 17.97, D_fake: 17.66\n",
            "e: 17\n",
            "pre_loss: 5.40, loss_ss: 3.12, loss_tt: 2.00, loss_adv: 0.27\n",
            "discriminator_loss: 0.32, D_real: 0.05, D_fake: 0.27\n",
            "e: 18\n",
            "pre_loss: 8.02, loss_ss: 3.16, loss_tt: 2.00, loss_adv: 2.83\n",
            "discriminator_loss: 1.67, D_real: 1.20, D_fake: 0.48\n",
            "e: 19\n",
            "pre_loss: 7.83, loss_ss: 4.02, loss_tt: 2.92, loss_adv: 0.79\n",
            "discriminator_loss: 0.10, D_real: 0.07, D_fake: 0.03\n",
            "e: 20\n",
            "pre_loss: 5.91, loss_ss: 3.38, loss_tt: 2.43, loss_adv: 0.05\n",
            "discriminator_loss: 0.90, D_real: 0.25, D_fake: 0.64\n",
            "e: 21\n",
            "pre_loss: 99.88, loss_ss: 3.38, loss_tt: 1.96, loss_adv: 94.51\n",
            "discriminator_loss: 252.04, D_real: 137.17, D_fake: 114.86\n",
            "e: 22\n",
            "pre_loss: 6.94, loss_ss: 3.11, loss_tt: 1.74, loss_adv: 2.07\n",
            "discriminator_loss: 0.86, D_real: 0.63, D_fake: 0.22\n",
            "e: 23\n",
            "pre_loss: 9.15, loss_ss: 4.57, loss_tt: 3.43, loss_adv: 1.02\n",
            "discriminator_loss: 0.13, D_real: 0.09, D_fake: 0.04\n",
            "e: 24\n",
            "pre_loss: 11.13, loss_ss: 3.72, loss_tt: 2.39, loss_adv: 4.97\n",
            "discriminator_loss: 20.43, D_real: 10.01, D_fake: 10.42\n",
            "e: 25\n",
            "pre_loss: 6.74, loss_ss: 3.17, loss_tt: 1.99, loss_adv: 1.57\n",
            "discriminator_loss: 0.40, D_real: 0.32, D_fake: 0.07\n",
            "e: 26\n",
            "pre_loss: 8.78, loss_ss: 4.13, loss_tt: 2.86, loss_adv: 1.71\n",
            "discriminator_loss: 4.38, D_real: 3.77, D_fake: 0.61\n",
            "e: 27\n",
            "pre_loss: 9.83, loss_ss: 4.16, loss_tt: 2.69, loss_adv: 2.90\n",
            "discriminator_loss: 2.59, D_real: 0.72, D_fake: 1.87\n",
            "e: 28\n",
            "pre_loss: 8.38, loss_ss: 4.27, loss_tt: 2.96, loss_adv: 1.10\n",
            "discriminator_loss: 0.62, D_real: 0.55, D_fake: 0.07\n",
            "e: 29\n",
            "pre_loss: 8.39, loss_ss: 4.29, loss_tt: 2.90, loss_adv: 1.14\n",
            "discriminator_loss: 0.24, D_real: 0.22, D_fake: 0.02\n",
            "e: 30\n",
            "pre_loss: 7.94, loss_ss: 4.23, loss_tt: 2.80, loss_adv: 0.87\n",
            "discriminator_loss: 0.09, D_real: 0.08, D_fake: 0.01\n",
            "e: 31\n",
            "pre_loss: 8.30, loss_ss: 4.09, loss_tt: 2.64, loss_adv: 1.53\n",
            "discriminator_loss: 0.19, D_real: 0.13, D_fake: 0.06\n",
            "e: 32\n",
            "pre_loss: 8.26, loss_ss: 3.91, loss_tt: 2.47, loss_adv: 1.83\n",
            "discriminator_loss: 0.41, D_real: 0.28, D_fake: 0.13\n",
            "e: 33\n",
            "pre_loss: 7.43, loss_ss: 3.65, loss_tt: 2.32, loss_adv: 1.42\n",
            "discriminator_loss: 0.19, D_real: 0.14, D_fake: 0.05\n",
            "e: 34\n",
            "pre_loss: 6.90, loss_ss: 3.49, loss_tt: 2.24, loss_adv: 1.14\n",
            "discriminator_loss: 0.07, D_real: 0.04, D_fake: 0.03\n",
            "e: 35\n",
            "pre_loss: 6.53, loss_ss: 3.29, loss_tt: 2.07, loss_adv: 1.14\n",
            "discriminator_loss: 0.06, D_real: 0.02, D_fake: 0.04\n",
            "e: 36\n",
            "pre_loss: 8.11, loss_ss: 4.22, loss_tt: 2.72, loss_adv: 1.10\n",
            "discriminator_loss: 0.16, D_real: 0.11, D_fake: 0.05\n",
            "e: 37\n",
            "pre_loss: 7.89, loss_ss: 4.17, loss_tt: 2.60, loss_adv: 1.07\n",
            "discriminator_loss: 0.04, D_real: 0.02, D_fake: 0.02\n",
            "e: 38\n",
            "pre_loss: 6.79, loss_ss: 4.14, loss_tt: 2.45, loss_adv: 0.16\n",
            "discriminator_loss: 4.00, D_real: 2.09, D_fake: 1.91\n",
            "e: 39\n",
            "pre_loss: 13.91, loss_ss: 3.97, loss_tt: 2.28, loss_adv: 7.62\n",
            "discriminator_loss: 30.48, D_real: 16.38, D_fake: 14.11\n",
            "e: 40\n",
            "pre_loss: 6.91, loss_ss: 3.72, loss_tt: 2.21, loss_adv: 0.93\n",
            "discriminator_loss: 0.02, D_real: 0.01, D_fake: 0.02\n",
            "e: 41\n",
            "pre_loss: 6.41, loss_ss: 3.48, loss_tt: 2.30, loss_adv: 0.60\n",
            "discriminator_loss: 0.09, D_real: 0.03, D_fake: 0.07\n",
            "e: 42\n",
            "pre_loss: 5.26, loss_ss: 3.12, loss_tt: 2.04, loss_adv: 0.09\n",
            "discriminator_loss: 0.94, D_real: 0.37, D_fake: 0.57\n",
            "e: 43\n",
            "pre_loss: 14.03, loss_ss: 3.05, loss_tt: 1.88, loss_adv: 9.09\n",
            "discriminator_loss: 9.09, D_real: 5.02, D_fake: 4.07\n",
            "e: 44\n",
            "pre_loss: 8.39, loss_ss: 3.01, loss_tt: 1.75, loss_adv: 3.61\n",
            "discriminator_loss: 1.94, D_real: 1.12, D_fake: 0.81\n",
            "e: 45\n",
            "pre_loss: 10.83, loss_ss: 2.95, loss_tt: 1.63, loss_adv: 6.23\n",
            "discriminator_loss: 4.93, D_real: 2.70, D_fake: 2.24\n",
            "e: 46\n",
            "pre_loss: 4.63, loss_ss: 2.85, loss_tt: 1.38, loss_adv: 0.39\n",
            "discriminator_loss: 0.20, D_real: 0.05, D_fake: 0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYMK4s7EdbsO"
      },
      "outputs": [],
      "source": [
        "'''Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "\n",
        "real_label = Variable(Tensor(batchSize).fill_(1.0), requires_grad=False)\n",
        "fake_label = Variable(Tensor(batchSize).fill_(0.0), requires_grad=False)\n",
        "real_label = real_label[:, None]\n",
        "fake_label = fake_label[:, None]\n",
        "\n",
        "\n",
        "input_A = Tensor(batchSize, 3, image_size, image_size)\n",
        "input_B = Tensor(batchSize, 3, image_size, image_size)\n",
        "\n",
        "for epoch in range(0, 200):\n",
        "    i = -1\n",
        "    for batchA, batchB in zip(target_train, source_train):\n",
        "        i+= 1\n",
        "        real_A = Variable(input_A.copy_(batchA[0]))\n",
        "        real_B = Variable(input_B.copy_(batchB[0]))\n",
        "\n",
        "        optimizer_pre.zero_grad()\n",
        "        mixed_image, reconstructionA, reconstructionB = perceptual(real_A, real_B)\n",
        "\n",
        "        loss_ss = criterion_construct(reconstructionA, real_A) * 15.0\n",
        "        loss_tt = criterion_construct(reconstructionB, real_B) * 15.0\n",
        "\n",
        "        TV_loss = total_variation_loss(mixed_image)\n",
        "        pred_fake = discriminator(mixed_image)\n",
        "        loss_adv = criterion_adv(pred_fake, real_label)\n",
        "\n",
        "        cuda_mixed_image = mixed_image.clone().requires_grad_(True).cuda()\n",
        "        cuda_real_A = real_A.clone().requires_grad_(True).cuda()\n",
        "        cuda_real_B = real_B.clone().requires_grad_(True).cuda()\n",
        "        style_features = get_features(cuda_real_A, vgg)\n",
        "        content_features = get_features(cuda_real_B, vgg)\n",
        "        target_features = get_features(cuda_mixed_image, vgg)\n",
        "\n",
        "        content_loss = compute_content_loss(target_features['conv4_2'], content_features['conv4_2']) * 0.1\n",
        "        style_loss = compute_style_loss(style_features, target_features) * 0.05\n",
        "\n",
        "        preceptual_loss = loss_ss + loss_tt + TV_loss + loss_adv + content_loss + style_loss\n",
        "\n",
        "        preceptual_loss.backward()\n",
        "        optimizer_pre.step()\n",
        "\n",
        "        optimizer_dis.zero_grad()\n",
        "\n",
        "        pred_real = discriminator(real_A)\n",
        "        pred_fake = discriminator(mixed_image.detach())\n",
        "\n",
        "        loss_dis_real = criterion_discriminator(pred_real, real_label)\n",
        "        loss_dis_fake = criterion_discriminator(pred_fake, fake_label)\n",
        "\n",
        "        discriminator_loss = loss_dis_real + loss_dis_fake\n",
        "        discriminator_loss.backward()\n",
        "        optimizer_dis.step()\n",
        "\n",
        "        if  i % 200 == 0:\n",
        "            real_A = real_A.data\n",
        "            real_B = real_B.data\n",
        "            mixed_image = mixed_image.data\n",
        "            reconstructionA = reconstructionA.data\n",
        "            reconstructionB = reconstructionB.data\n",
        "\n",
        "            save_image(real_A, './output/%03d/%d_A.png' % ( epoch, i))\n",
        "            save_image(real_B, 'output/%03d/%d_B.png' % ( epoch, i))\n",
        "            save_image(reconstructionA, 'output/%03d/%d_reconA.png' % ( epoch, i))\n",
        "            save_image(reconstructionB, 'output/%03d/%d_reconB.png' % ( epoch, i))\n",
        "            save_image(mixed_image, 'output/%03d/%d_Mixed.png' % ( epoch, i))\n",
        "\n",
        "    print (\"e: %d\" % epoch)    \n",
        "    print (\"pre_loss: %.2f, loss_ss: %.2f, loss_tt: %.2f, loss_adv: %.2f, content: %.2f, style: %.2f\" % (preceptual_loss, loss_ss, loss_tt, loss_adv, content_loss, style_loss))\n",
        "    print (\"discriminator_loss: %.2f, D_real: %.2f, D_fake: %.2f\" % (discriminator_loss, loss_dis_real, loss_dis_fake))\n",
        "\n",
        "    scheduler_pre.step()\n",
        "    scheduler_dis.step()\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAcUzkPxdbsO"
      },
      "outputs": [],
      "source": [
        "torch.save(perceptual, \"./model_weight/perceptual\")\n",
        "torch.save(discriminator, \"./model_weight/discriminator\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resulting image"
      ],
      "metadata": {
        "id": "3mtbjZVKba5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = imread('./output/199/400_Mixed.png')\n",
        "\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "5zOTBBi04khW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = imread('./output/199/400_reconB.png')\n",
        "\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "qKxxfZPF4pBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "my_model.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0abf52f7dff1bbf2191b90c10bb43e97e891f8d70dafe2d0c71717742c591866"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}