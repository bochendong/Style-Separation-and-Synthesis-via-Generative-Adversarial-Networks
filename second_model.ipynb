{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "tAz7koKv1MB5"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision import transforms, models, datasets\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from pylab import imread,subplot,imshow,show\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR, StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2i0ul1TdfDUu"
      },
      "outputs": [],
      "source": [
        "for epoch in range (200):\n",
        "  if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "    os.mkdir(\"./output/%03d\" % epoch)\n",
        "  else:\n",
        "    files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "    for f in files:\n",
        "      os.remove(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUJIfmDn1MCx"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True}\n",
        "\n",
        "cuda = True\n",
        "image_size = 32\n",
        "batchSize = 16"
      ],
      "metadata": {
        "id": "50aUu3MfiUvQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_img(source, target, source_label, target_label):\n",
        "    num_row = 4\n",
        "    num_col = 5\n",
        "    num = 10\n",
        "    images = source[:num]\n",
        "    labels = source_label[:num]\n",
        "\n",
        "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
        "    for i in range(num):\n",
        "        ax = axes[i//num_col, i%num_col]\n",
        "        \n",
        "        image =  images[i].transpose(0,2).transpose(0,1)\n",
        "\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title('Label: {}'.format(labels[i]))\n",
        "\n",
        "\n",
        "    images = target[:num]\n",
        "    labels = target_label[:num]\n",
        "    for i in range(10,20):\n",
        "        ax = axes[i//num_col, i%num_col]\n",
        "        image = images[i - 10].transpose(0,2).transpose(0,1)\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title('Label: {}'.format(labels[i - 10]))\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "osdNOtN2mpk0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mnist"
      ],
      "metadata": {
        "id": "kluPV1H5oETL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.Grayscale(3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda t: t * 2 - 1)])\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='./data/mnist', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data/mnist', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "OaqWTwLNn4Xa"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mnist-M"
      ],
      "metadata": {
        "id": "QkU7PlPEoHUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_backgrounds():\n",
        "    backgrounds = []\n",
        "    for file in os.listdir(\"./images/train\"):\n",
        "        if file.endswith('.jpg'):\n",
        "            backgrounds.append(plt.imread(os.path.join(\"./images/train\",file)))\n",
        "    return backgrounds\n",
        "\n",
        "def compose_image(image, backgrounds):\n",
        "    image = (image > 0).astype(np.float32)\n",
        "    image = image.reshape([28,28])*255.0\n",
        "    \n",
        "    image = np.stack([image,image,image],axis=2)\n",
        "    \n",
        "    background = np.random.choice(backgrounds)\n",
        "    w,h,_ = background.shape\n",
        "    dw, dh,_ = image.shape\n",
        "    x = np.random.randint(0,w-dw)\n",
        "    y = np.random.randint(0,h-dh)\n",
        "    \n",
        "    temp = background[x:x+dw, y:y+dh]\n",
        "    return np.abs(temp-image).astype(np.uint8)\n",
        "\n",
        "class MNISTM(Dataset):    \n",
        "    def __init__(self, train=True,transform=None):\n",
        "        if train:\n",
        "            self.data = datasets.MNIST(root='.data/mnist',train=True, download=True)\n",
        "        else:\n",
        "            self.data = datasets.MNIST(root='.data/mnist',train=False, download=True)\n",
        "        self.backgrounds = get_backgrounds()\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.targets = []\n",
        "        for index in range(len(self.data)):\n",
        "            image = np.array(self.data.__getitem__(index)[0])\n",
        "            target = self.data.__getitem__(index)[1]\n",
        "            image = compose_image(image, self.backgrounds)\n",
        "            if self.transform is not None:\n",
        "                image = self.transform(image)\n",
        "            self.images.append(image)\n",
        "            self.targets.append(target)\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        image = self.images[index]\n",
        "        target = self.targets[index]\n",
        "        \n",
        "        return image, target\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "fNrMH4z5oGtu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: t * 2 - 1)\n",
        "        ])\n",
        "\n",
        "trainset = MNISTM(train=True,transform=transform)\n",
        "testset = MNISTM(train=False,transform=transform)"
      ],
      "metadata": {
        "id": "XXagwGNxov2t",
        "outputId": "b01dbbf9-3335-43f0-e4cc-9d038b66afc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_train = DataLoader(mnist_trainset, batch_size=batchSize, shuffle=True, drop_last=True, **kwargs)\n",
        "source_test = DataLoader(mnist_testset, batch_size=batchSize, shuffle=True, drop_last=True, **kwargs)"
      ],
      "metadata": {
        "id": "BRcXrUIZv6CF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_train = DataLoader(trainset, batch_size=batchSize, shuffle=True, drop_last=True, **kwargs)\n",
        "target_test = DataLoader(testset, batch_size=batchSize, shuffle=False, drop_last=True, **kwargs)"
      ],
      "metadata": {
        "id": "-IVxtlhAoyRU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_iter = iter(source_train)\n",
        "source_inputs, source_label = source_iter.next()\n",
        "\n",
        "target_iter = iter(target_train)\n",
        "target_inputs, target_label = target_iter.next()"
      ],
      "metadata": {
        "id": "a9w67Lw0mrOY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%2f, %2f\" % (torch.min(source_inputs), torch.max(source_inputs)))"
      ],
      "metadata": {
        "id": "Ht_5WRfiryXC",
        "outputId": "76d83be7-18b7-429a-b962-16ce213a7556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.000000, 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%2f, %2f\" % (torch.min(target_inputs), torch.max(target_inputs)))"
      ],
      "metadata": {
        "id": "WtYBA8tBr-dX",
        "outputId": "8e1e86df-df78-4490-c7b3-a25988e2d022",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.000000, 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = random.randint(0, 15)\n",
        "image_source_sample = source_inputs[index]\n",
        "image_source_sample = (image_source_sample + 1) * 0.5\n",
        "image = image_source_sample.transpose(0,2).transpose(0,1)\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "ycvMXFa7cihR",
        "outputId": "4c74eb98-ce6e-47ea-a4c1-c55ca78b7536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5020a27910>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_target_sample = target_inputs[index]\n",
        "image_target_sample = (image_target_sample + 1) * 0.5\n",
        "image = image_target_sample.transpose(0,2).transpose(0,1)\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "RVS5SkcFdS3Q",
        "outputId": "b2a2b861-424a-4227-ee23-7138d75a5422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5020916150>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj4IrxmY1MDH"
      },
      "source": [
        "# Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kV1qBY41MDO"
      },
      "source": [
        "## Perceptual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "NFsKAlmUdbsH"
      },
      "outputs": [],
      "source": [
        "class Perceptual(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Perceptual, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(3, 64, 7),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(64, 3, 7),\n",
        "            nn.InstanceNorm2d(3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, A, B):\n",
        "        encode_A = self.encoder(A)\n",
        "        encode_B = self.encoder(B)\n",
        "        reconA = self.decoder(encode_A)\n",
        "        reconB = self.decoder(encode_B)\n",
        "\n",
        "        encode_A.detach()\n",
        "        encode_B.detach()\n",
        "\n",
        "        style = encode_A[:, 0:128, : , :]\n",
        "        content = encode_B[:, 128:256, :, :]\n",
        "        \n",
        "        mixed_latent = torch.cat([style, content], dim=1)\n",
        "        mixed_image = self.decoder(mixed_latent)\n",
        "\n",
        "        return mixed_image, reconA, reconB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tensor_source = source_inputs\n",
        "test_tensor_target = target_inputs\n",
        "\n",
        "perceptual = Perceptual()\n",
        "encoder = perceptual.encoder\n",
        "decoder = perceptual.decoder\n",
        "encoder_out = encoder(test_tensor_target)\n",
        "decoder_out = decoder(encoder_out)\n",
        "\n",
        "print(encoder_out.size())\n",
        "print(decoder_out.size())\n",
        "\n",
        "mixed, reconA, reconB = perceptual(test_tensor_source, test_tensor_target)\n",
        "print(mixed.size())\n",
        "print(reconA.size())\n",
        "print(reconB.size())\n",
        "\n",
        "print(\"mixed: min: %.2f, max: %.2f \" % (torch.min(mixed), torch.max(mixed)))\n",
        "\n",
        "print(\"reconA: min: %.2f, max: %.2f \" % (torch.min(reconA), torch.max(reconA)))\n",
        "\n",
        "print(\"reconB: min: %.2f, max: %.2f \" % (torch.min(reconB), torch.max(reconB)))"
      ],
      "metadata": {
        "id": "ApdMclXUeRUC",
        "outputId": "9d395016-d179-444a-dc17-3510176f0990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 256, 8, 8])\n",
            "torch.Size([16, 3, 32, 32])\n",
            "torch.Size([16, 3, 32, 32])\n",
            "torch.Size([16, 3, 32, 32])\n",
            "torch.Size([16, 3, 32, 32])\n",
            "mixed: min: -1.00, max: 1.00 \n",
            "reconA: min: -1.00, max: 1.00 \n",
            "reconB: min: -1.00, max: 1.00 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mixed[0][0][0] - reconA[0][0][0])\n",
        "print(mixed[0][0][0] - reconB[0][0][0])"
      ],
      "metadata": {
        "id": "M6NIU0nTgYJx",
        "outputId": "f000b387-6e34-468b-80c3-a0ba1ed562ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0099,  0.3845, -0.1854, -0.8284,  0.0756,  0.1412, -0.1317,  0.3071,\n",
            "         0.7307,  0.3407, -0.3687,  0.0293,  0.2705,  0.7862, -0.4630, -0.6915,\n",
            "         0.8435,  0.1458,  0.4327, -0.4140, -0.5035,  0.0301,  0.1885,  0.1583,\n",
            "         0.3218,  0.4494,  0.2259,  0.0106,  0.3904,  0.5153,  0.1561,  0.3694],\n",
            "       grad_fn=<SubBackward0>)\n",
            "tensor([-0.3743,  0.6743, -0.7067, -0.1883,  0.2838, -0.4856, -0.4541, -0.1170,\n",
            "         0.6849,  0.6962, -0.9130,  0.3464,  0.0800,  0.3866, -0.4016, -0.3656,\n",
            "        -0.4752, -0.1715,  1.0259, -0.3761, -0.2827, -0.0826, -0.1782,  0.0661,\n",
            "        -0.4575, -0.2132, -0.0700,  0.3756,  0.2064,  0.2126, -0.2967, -0.0246],\n",
            "       grad_fn=<SubBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLubC4sJ1MDd"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "67DdNmWP1MDd"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc = 3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, padding=1),\n",
        "            nn.InstanceNorm2d(128), \n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, padding=1),\n",
        "            nn.InstanceNorm2d(256), \n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  self.model(x)\n",
        "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator()\n",
        "\n",
        "output = discriminator(test_tensor_source)\n",
        "print(output.size())\n",
        "print(torch.min(output))\n",
        "print(torch.max(output))"
      ],
      "metadata": {
        "id": "SY5fBnHMgAYz",
        "outputId": "e17c947e-0b53-449d-9cc8-c92473f8ab17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1])\n",
            "tensor(0.1058, grad_fn=<MinBackward1>)\n",
            "tensor(0.1290, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPaeTfIX1MDk"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "6EkNLO_T1MDk"
      },
      "outputs": [],
      "source": [
        "def tv_loss(img, tv_weight=5e-2):\n",
        "    w_variance = torch.sum(torch.pow(img[:,:,:,:-1] - img[:,:,:,1:], 2))\n",
        "    h_variance = torch.sum(torch.pow(img[:,:,:-1,:] - img[:,:,1:,:], 2))\n",
        "    loss = tv_weight * (h_variance + w_variance)\n",
        "    return loss\n",
        "\n",
        "def total_variation_loss(img, weight=5e-2):\n",
        "    bs_img, c_img, h_img, w_img = img.size()\n",
        "    tv_h = torch.pow(img[:, :, 1:, :] - img[:, :, :-1, :], 2).sum()\n",
        "    tv_w = torch.pow(img[:, :, :, 1:] - img[:, :, :, :-1], 2).sum()\n",
        "    return weight * (tv_h + tv_w) / (bs_img * c_img * h_img * w_img)\n",
        "\n",
        "def compute_content_loss(target_feature, content_feature):\n",
        "    return torch.mean((target_feature - content_feature)**2)\n",
        "\n",
        "def batch_gram_matrix(img):\n",
        "    b, d, h, w = img.size()\n",
        "    img = img.view(b*d, h*w)\n",
        "    gram = torch.mm(img, img.t())\n",
        "    return gram\n",
        "    \n",
        "style_weights = {'conv1_1': 1.0, 'conv2_1': 0.75, 'conv3_1': 0.2, 'conv4_1': 0.2, 'conv5_1': 0.2}\n",
        "\n",
        "def compute_style_loss(style_features, target_features):\n",
        "    style_loss = 0\n",
        "    style_grams = {layer: batch_gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "    for layer in style_weights:\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = batch_gram_matrix(target_feature)\n",
        "        _, d, h, w = target_feature.shape\n",
        "\n",
        "        style_gram = style_grams[layer]\n",
        "\n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n",
        "        style_loss += layer_style_loss / (d * h * w)\n",
        "\n",
        "    return style_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEujnWnX1MDl"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4U1o05Y1MDm"
      },
      "source": [
        "## Training Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_L7ExZCq1MDz"
      },
      "outputs": [],
      "source": [
        "def get_features(image, model, layers=None):\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1', \n",
        "              '3': 'conv2_1', \n",
        "              '6': 'conv3_1', \n",
        "              '11': 'conv4_1',\n",
        "              '13': 'conv4_2', \n",
        "              '16': 'conv5_1'}\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVvM_If_1MD1"
      },
      "source": [
        "## Model hypermeter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "TBdlE72j1MD4"
      },
      "outputs": [],
      "source": [
        "perceptual = Perceptual()\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Y8zIpzSL1MD5"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.05\n",
        "beta = (0.5, 0.999)\n",
        "\n",
        "criterion_adv = torch.nn.MSELoss()\n",
        "criterion_discriminator = torch.nn.MSELoss()\n",
        "criterion_construct = torch.nn.L1Loss()\n",
        "\n",
        "optimizer_pre = torch.optim.Adam(perceptual.parameters(), lr=learning_rate, betas=beta)\n",
        "optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=beta)\n",
        "\n",
        "scheduler_pre = StepLR(optimizer_pre, step_size=40, gamma=0.4)\n",
        "scheduler_dis = StepLR(optimizer_dis, step_size=40, gamma=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "f7f15cde5f634963a0b99c5e56853187",
            "f9de528844184af391607c2a2b0bdf34",
            "a804f7f455ba4ed1b3f434584d639c63",
            "9acab382674d4a49a17ea878306cba5c",
            "44cb3e79acc54c9fb062364f3c73efad",
            "8209b4880d7e4b32a44d985573648030",
            "aae6a9c29efd4d3aa23d7588d381cb90",
            "35d513648d294b78b3eb05ab16c4a2ce",
            "953487c6c75b4fba848b9f16586ac16b",
            "7f94d0dc647d4403afd35351cf28b37c",
            "9a77e91128f44650929dacd97cd72576"
          ]
        },
        "id": "OCYbEweS1MD5",
        "outputId": "8c178ad1-e44b-4896-84b5-5c7356e2234a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to /root/.cache/torch/hub/checkpoints/vgg11-8a719046.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/507M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7f15cde5f634963a0b99c5e56853187"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "vgg = models.vgg11(pretrained=True).features\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "1_h8u5-V1MD5"
      },
      "outputs": [],
      "source": [
        "if (torch.cuda.is_available()):\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    criterion_construct = criterion_construct.cuda()\n",
        "    criterion_discriminator = criterion_discriminator.cuda()\n",
        "    criterion_construct = criterion_construct.cuda()\n",
        "\n",
        "    vgg.cuda()\n",
        "    perceptual.cuda()\n",
        "    discriminator.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe87ze5D1MD6"
      },
      "source": [
        "## Init the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "vVGb29b_1MD6"
      },
      "outputs": [],
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant(m.bias.data, 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-cEljb71MD6",
        "outputId": "ecdf3827-2d4f-4ca6-a785-914e26c0d5d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "perceptual.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j-yi9aD1MD7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYMK4s7EdbsO",
        "outputId": "66e18c05-0c46-4090-b159-77d445b9e35b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e: 0\n",
            "pre_loss: 22.83, loss_ss: 8.04, loss_tt: 13.01, loss_adv: 1.33, content: 0.33, style: 0.10\n",
            "discriminator_loss: 8.39, D_real: 2.22, D_fake: 6.17\n",
            "e: 1\n",
            "pre_loss: 25.10, loss_ss: 9.33, loss_tt: 13.38, loss_adv: 0.96, content: 0.56, style: 0.83\n",
            "discriminator_loss: 2.85, D_real: 1.56, D_fake: 1.29\n",
            "e: 2\n",
            "pre_loss: 28.24, loss_ss: 7.51, loss_tt: 11.19, loss_adv: 9.00, content: 0.45, style: 0.05\n",
            "discriminator_loss: 17.18, D_real: 5.72, D_fake: 11.46\n",
            "e: 3\n",
            "pre_loss: 24.09, loss_ss: 9.65, loss_tt: 13.99, loss_adv: 0.06, content: 0.30, style: 0.02\n",
            "discriminator_loss: 0.71, D_real: 0.33, D_fake: 0.38\n",
            "e: 4\n",
            "pre_loss: 17.24, loss_ss: 6.39, loss_tt: 9.95, loss_adv: 0.41, content: 0.39, style: 0.05\n",
            "discriminator_loss: 0.54, D_real: 0.39, D_fake: 0.15\n",
            "e: 5\n",
            "pre_loss: 27.93, loss_ss: 6.68, loss_tt: 11.77, loss_adv: 8.91, content: 0.38, style: 0.12\n",
            "discriminator_loss: 23.12, D_real: 8.20, D_fake: 14.92\n",
            "e: 6\n",
            "pre_loss: 22.02, loss_ss: 8.42, loss_tt: 13.16, loss_adv: 0.04, content: 0.29, style: 0.01\n",
            "discriminator_loss: 0.93, D_real: 0.39, D_fake: 0.55\n",
            "e: 7\n",
            "pre_loss: 17.47, loss_ss: 5.84, loss_tt: 11.20, loss_adv: 0.00, content: 0.34, style: 0.05\n",
            "discriminator_loss: 0.87, D_real: 0.01, D_fake: 0.86\n",
            "e: 8\n",
            "pre_loss: 19.68, loss_ss: 6.81, loss_tt: 12.43, loss_adv: 0.07, content: 0.30, style: 0.03\n",
            "discriminator_loss: 1.34, D_real: 0.30, D_fake: 1.05\n",
            "e: 9\n",
            "pre_loss: 12.86, loss_ss: 3.84, loss_tt: 8.77, loss_adv: 0.16, content: 0.07, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.18, D_fake: 0.33\n",
            "e: 10\n",
            "pre_loss: 12.52, loss_ss: 3.80, loss_tt: 8.42, loss_adv: 0.12, content: 0.16, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.26, D_fake: 0.25\n",
            "e: 11\n",
            "pre_loss: 12.29, loss_ss: 2.39, loss_tt: 9.55, loss_adv: 0.29, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.32, D_fake: 0.19\n",
            "e: 12\n",
            "pre_loss: 18.35, loss_ss: 5.81, loss_tt: 11.89, loss_adv: 0.17, content: 0.37, style: 0.07\n",
            "discriminator_loss: 0.52, D_real: 0.17, D_fake: 0.34\n",
            "e: 13\n",
            "pre_loss: 11.63, loss_ss: 3.03, loss_tt: 8.17, loss_adv: 0.37, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.52, D_real: 0.37, D_fake: 0.15\n",
            "e: 14\n",
            "pre_loss: 11.23, loss_ss: 2.67, loss_tt: 8.26, loss_adv: 0.24, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 15\n",
            "pre_loss: 54.76, loss_ss: 2.79, loss_tt: 8.20, loss_adv: 43.72, content: 0.04, style: 0.01\n",
            "discriminator_loss: 101.48, D_real: 43.63, D_fake: 57.85\n",
            "e: 16\n",
            "pre_loss: 12.32, loss_ss: 3.25, loss_tt: 8.76, loss_adv: 0.27, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.28, D_fake: 0.22\n",
            "e: 17\n",
            "pre_loss: 12.62, loss_ss: 2.43, loss_tt: 8.32, loss_adv: 1.80, content: 0.05, style: 0.01\n",
            "discriminator_loss: 3.52, D_real: 2.99, D_fake: 0.54\n",
            "e: 18\n",
            "pre_loss: 10.72, loss_ss: 2.34, loss_tt: 8.07, loss_adv: 0.25, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.30, D_fake: 0.21\n",
            "e: 19\n",
            "pre_loss: 10.66, loss_ss: 2.54, loss_tt: 7.95, loss_adv: 0.08, content: 0.06, style: 0.01\n",
            "discriminator_loss: 0.54, D_real: 0.13, D_fake: 0.40\n",
            "e: 20\n",
            "pre_loss: 21.29, loss_ss: 8.31, loss_tt: 12.50, loss_adv: 0.08, content: 0.33, style: 0.03\n",
            "discriminator_loss: 1.16, D_real: 0.78, D_fake: 0.38\n",
            "e: 21\n",
            "pre_loss: 18.66, loss_ss: 6.42, loss_tt: 11.75, loss_adv: 0.07, content: 0.36, style: 0.03\n",
            "discriminator_loss: 0.73, D_real: 0.28, D_fake: 0.45\n",
            "e: 22\n",
            "pre_loss: 10.35, loss_ss: 1.98, loss_tt: 8.15, loss_adv: 0.15, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.58, D_real: 0.49, D_fake: 0.09\n",
            "e: 23\n",
            "pre_loss: 18.91, loss_ss: 6.56, loss_tt: 11.66, loss_adv: 0.20, content: 0.37, style: 0.07\n",
            "discriminator_loss: 0.76, D_real: 0.52, D_fake: 0.25\n",
            "e: 24\n",
            "pre_loss: 11.87, loss_ss: 2.67, loss_tt: 8.62, loss_adv: 0.49, content: 0.07, style: 0.01\n",
            "discriminator_loss: 1.12, D_real: 1.11, D_fake: 0.01\n",
            "e: 25\n",
            "pre_loss: 21.98, loss_ss: 8.94, loss_tt: 11.99, loss_adv: 0.52, content: 0.42, style: 0.04\n",
            "discriminator_loss: 1.96, D_real: 1.36, D_fake: 0.61\n",
            "e: 26\n",
            "pre_loss: 12.13, loss_ss: 2.86, loss_tt: 8.63, loss_adv: 0.57, content: 0.06, style: 0.01\n",
            "discriminator_loss: 1.46, D_real: 1.42, D_fake: 0.04\n",
            "e: 27\n",
            "pre_loss: 10.48, loss_ss: 2.84, loss_tt: 7.55, loss_adv: 0.03, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.54, D_real: 0.41, D_fake: 0.14\n",
            "e: 28\n",
            "pre_loss: 15.81, loss_ss: 3.41, loss_tt: 8.73, loss_adv: 3.54, content: 0.11, style: 0.01\n",
            "discriminator_loss: 8.29, D_real: 2.19, D_fake: 6.10\n",
            "e: 29\n",
            "pre_loss: 11.09, loss_ss: 2.63, loss_tt: 8.40, loss_adv: 0.01, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.53, D_real: 0.16, D_fake: 0.37\n",
            "e: 30\n",
            "pre_loss: 12.48, loss_ss: 3.64, loss_tt: 8.07, loss_adv: 0.56, content: 0.20, style: 0.01\n",
            "discriminator_loss: 1.29, D_real: 1.27, D_fake: 0.03\n",
            "e: 31\n",
            "pre_loss: 11.24, loss_ss: 2.78, loss_tt: 8.39, loss_adv: 0.01, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.55, D_real: 0.12, D_fake: 0.42\n",
            "e: 32\n",
            "pre_loss: 12.63, loss_ss: 3.44, loss_tt: 9.07, loss_adv: 0.01, content: 0.09, style: 0.01\n",
            "discriminator_loss: 0.55, D_real: 0.14, D_fake: 0.41\n",
            "e: 33\n",
            "pre_loss: 10.29, loss_ss: 2.13, loss_tt: 8.10, loss_adv: 0.01, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.53, D_real: 0.14, D_fake: 0.39\n",
            "e: 34\n",
            "pre_loss: 11.30, loss_ss: 2.70, loss_tt: 8.36, loss_adv: 0.14, content: 0.08, style: 0.01\n",
            "discriminator_loss: 0.61, D_real: 0.53, D_fake: 0.08\n",
            "e: 35\n",
            "pre_loss: 11.40, loss_ss: 2.23, loss_tt: 9.10, loss_adv: 0.01, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.27, D_fake: 0.23\n",
            "e: 36\n",
            "pre_loss: 12.15, loss_ss: 1.94, loss_tt: 8.26, loss_adv: 1.88, content: 0.05, style: 0.01\n",
            "discriminator_loss: 4.67, D_real: 0.89, D_fake: 3.78\n",
            "e: 37\n",
            "pre_loss: 10.71, loss_ss: 2.72, loss_tt: 7.85, loss_adv: 0.07, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.54, D_real: 0.42, D_fake: 0.12\n",
            "e: 38\n",
            "pre_loss: 10.79, loss_ss: 2.15, loss_tt: 8.43, loss_adv: 0.15, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.71, D_real: 0.68, D_fake: 0.04\n",
            "e: 39\n",
            "pre_loss: 11.37, loss_ss: 2.72, loss_tt: 8.56, loss_adv: 0.02, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.27, D_fake: 0.23\n",
            "e: 40\n",
            "pre_loss: 10.80, loss_ss: 2.53, loss_tt: 8.20, loss_adv: 0.00, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.22, D_fake: 0.28\n",
            "e: 41\n",
            "pre_loss: 9.91, loss_ss: 1.80, loss_tt: 8.06, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.26\n",
            "e: 42\n",
            "pre_loss: 10.40, loss_ss: 2.32, loss_tt: 8.02, loss_adv: 0.01, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.28, D_fake: 0.22\n",
            "e: 43\n",
            "pre_loss: 11.13, loss_ss: 2.40, loss_tt: 8.67, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.27\n",
            "e: 44\n",
            "pre_loss: 10.53, loss_ss: 2.42, loss_tt: 8.07, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.23, D_fake: 0.27\n",
            "e: 45\n",
            "pre_loss: 10.12, loss_ss: 1.84, loss_tt: 8.22, loss_adv: 0.01, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.26, D_fake: 0.24\n",
            "e: 46\n",
            "pre_loss: 10.71, loss_ss: 2.12, loss_tt: 8.54, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 47\n",
            "pre_loss: 9.90, loss_ss: 1.96, loss_tt: 7.88, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.26, D_fake: 0.24\n",
            "e: 48\n",
            "pre_loss: 10.44, loss_ss: 2.25, loss_tt: 8.15, loss_adv: 0.00, content: 0.02, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.20, D_fake: 0.30\n",
            "e: 49\n",
            "pre_loss: 11.19, loss_ss: 2.55, loss_tt: 8.49, loss_adv: 0.08, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.55, D_real: 0.43, D_fake: 0.12\n",
            "e: 50\n",
            "pre_loss: 10.05, loss_ss: 2.09, loss_tt: 7.89, loss_adv: 0.02, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.31, D_fake: 0.19\n",
            "e: 51\n",
            "pre_loss: 9.79, loss_ss: 1.98, loss_tt: 7.76, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.20, D_fake: 0.31\n",
            "e: 52\n",
            "pre_loss: 10.03, loss_ss: 2.14, loss_tt: 7.84, loss_adv: 0.01, content: 0.02, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.33, D_fake: 0.18\n",
            "e: 53\n",
            "pre_loss: 10.76, loss_ss: 2.26, loss_tt: 8.40, loss_adv: 0.05, content: 0.02, style: 0.01\n",
            "discriminator_loss: 0.56, D_real: 0.45, D_fake: 0.11\n",
            "e: 54\n",
            "pre_loss: 10.21, loss_ss: 1.88, loss_tt: 8.27, loss_adv: 0.01, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 55\n",
            "pre_loss: 9.94, loss_ss: 1.78, loss_tt: 8.09, loss_adv: 0.03, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.57, D_real: 0.10, D_fake: 0.47\n",
            "e: 56\n",
            "pre_loss: 10.70, loss_ss: 2.44, loss_tt: 8.15, loss_adv: 0.03, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.28, D_fake: 0.22\n",
            "e: 57\n",
            "pre_loss: 10.68, loss_ss: 2.20, loss_tt: 8.40, loss_adv: 0.01, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.28, D_fake: 0.23\n",
            "e: 58\n",
            "pre_loss: 10.38, loss_ss: 2.22, loss_tt: 8.12, loss_adv: 0.00, content: 0.02, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.20, D_fake: 0.31\n",
            "e: 59\n",
            "pre_loss: 10.27, loss_ss: 1.98, loss_tt: 8.00, loss_adv: 0.22, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.28, D_fake: 0.22\n",
            "e: 60\n",
            "pre_loss: 10.88, loss_ss: 2.55, loss_tt: 8.28, loss_adv: 0.00, content: 0.02, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.21, D_fake: 0.30\n",
            "e: 61\n",
            "pre_loss: 10.59, loss_ss: 2.45, loss_tt: 8.02, loss_adv: 0.05, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.52, D_real: 0.36, D_fake: 0.16\n",
            "e: 62\n",
            "pre_loss: 10.15, loss_ss: 2.27, loss_tt: 7.83, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 63\n",
            "pre_loss: 10.55, loss_ss: 2.32, loss_tt: 8.17, loss_adv: 0.00, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.18, D_fake: 0.33\n",
            "e: 64\n",
            "pre_loss: 10.27, loss_ss: 2.30, loss_tt: 7.91, loss_adv: 0.00, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 65\n",
            "pre_loss: 10.25, loss_ss: 2.17, loss_tt: 8.02, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.23, D_fake: 0.27\n",
            "e: 66\n",
            "pre_loss: 10.08, loss_ss: 1.84, loss_tt: 8.19, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.53, D_real: 0.15, D_fake: 0.38\n",
            "e: 67\n",
            "pre_loss: 11.09, loss_ss: 2.74, loss_tt: 8.29, loss_adv: 0.01, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.26, D_fake: 0.24\n",
            "e: 68\n",
            "pre_loss: 10.17, loss_ss: 2.03, loss_tt: 8.07, loss_adv: 0.00, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.20, D_fake: 0.30\n",
            "e: 69\n",
            "pre_loss: 10.85, loss_ss: 2.46, loss_tt: 8.32, loss_adv: 0.00, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.28, D_fake: 0.22\n",
            "e: 70\n",
            "pre_loss: 10.01, loss_ss: 2.11, loss_tt: 7.84, loss_adv: 0.00, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.26, D_fake: 0.24\n",
            "e: 71\n",
            "pre_loss: 10.90, loss_ss: 2.44, loss_tt: 8.39, loss_adv: 0.01, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.26, D_fake: 0.24\n",
            "e: 72\n",
            "pre_loss: 10.87, loss_ss: 2.26, loss_tt: 8.54, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.52, D_real: 0.17, D_fake: 0.35\n",
            "e: 73\n",
            "pre_loss: 10.81, loss_ss: 2.12, loss_tt: 8.52, loss_adv: 0.09, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 74\n",
            "pre_loss: 10.39, loss_ss: 2.38, loss_tt: 7.91, loss_adv: 0.02, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 75\n",
            "pre_loss: 10.54, loss_ss: 2.44, loss_tt: 8.04, loss_adv: 0.00, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.27, D_fake: 0.23\n",
            "e: 76\n",
            "pre_loss: 10.26, loss_ss: 1.85, loss_tt: 8.35, loss_adv: 0.01, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.28, D_fake: 0.22\n",
            "e: 77\n",
            "pre_loss: 10.13, loss_ss: 2.33, loss_tt: 7.73, loss_adv: 0.01, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 78\n",
            "pre_loss: 11.30, loss_ss: 3.06, loss_tt: 8.01, loss_adv: 0.13, content: 0.08, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.28, D_fake: 0.23\n",
            "e: 79\n",
            "pre_loss: 10.27, loss_ss: 1.93, loss_tt: 8.29, loss_adv: 0.01, content: 0.02, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.31, D_fake: 0.19\n",
            "e: 80\n",
            "pre_loss: 10.39, loss_ss: 2.21, loss_tt: 8.12, loss_adv: 0.01, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 81\n",
            "pre_loss: 10.87, loss_ss: 2.20, loss_tt: 8.58, loss_adv: 0.03, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.27, D_fake: 0.24\n",
            "e: 82\n",
            "pre_loss: 10.67, loss_ss: 2.12, loss_tt: 8.48, loss_adv: 0.01, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 83\n",
            "pre_loss: 11.01, loss_ss: 2.65, loss_tt: 8.27, loss_adv: 0.02, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.22, D_fake: 0.29\n",
            "e: 84\n",
            "pre_loss: 10.69, loss_ss: 2.23, loss_tt: 8.35, loss_adv: 0.02, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 85\n",
            "pre_loss: 10.11, loss_ss: 1.74, loss_tt: 8.25, loss_adv: 0.03, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 86\n",
            "pre_loss: 10.26, loss_ss: 1.48, loss_tt: 8.66, loss_adv: 0.04, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 87\n",
            "pre_loss: 11.40, loss_ss: 2.63, loss_tt: 8.62, loss_adv: 0.07, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.32, D_fake: 0.19\n",
            "e: 88\n",
            "pre_loss: 11.11, loss_ss: 2.20, loss_tt: 8.80, loss_adv: 0.02, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.19, D_fake: 0.32\n",
            "e: 89\n",
            "pre_loss: 10.84, loss_ss: 2.32, loss_tt: 8.39, loss_adv: 0.04, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 90\n",
            "pre_loss: 10.35, loss_ss: 1.92, loss_tt: 8.24, loss_adv: 0.09, content: 0.06, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.30, D_fake: 0.21\n",
            "e: 91\n",
            "pre_loss: 9.83, loss_ss: 1.66, loss_tt: 8.00, loss_adv: 0.08, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.52, D_real: 0.35, D_fake: 0.17\n",
            "e: 92\n",
            "pre_loss: 10.03, loss_ss: 1.71, loss_tt: 8.24, loss_adv: 0.01, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 93\n",
            "pre_loss: 11.17, loss_ss: 2.35, loss_tt: 8.73, loss_adv: 0.01, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.17, D_fake: 0.34\n",
            "e: 94\n",
            "pre_loss: 10.66, loss_ss: 2.07, loss_tt: 8.48, loss_adv: 0.03, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.26, D_fake: 0.24\n",
            "e: 95\n",
            "pre_loss: 10.88, loss_ss: 2.05, loss_tt: 8.71, loss_adv: 0.03, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.27, D_fake: 0.23\n",
            "e: 96\n",
            "pre_loss: 10.00, loss_ss: 1.75, loss_tt: 8.13, loss_adv: 0.04, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 97\n",
            "pre_loss: 11.27, loss_ss: 2.55, loss_tt: 8.55, loss_adv: 0.08, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.33, D_fake: 0.18\n",
            "e: 98\n",
            "pre_loss: 10.16, loss_ss: 2.18, loss_tt: 7.83, loss_adv: 0.06, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 99\n",
            "pre_loss: 11.14, loss_ss: 2.37, loss_tt: 8.61, loss_adv: 0.06, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.26, D_fake: 0.24\n",
            "e: 100\n",
            "pre_loss: 11.01, loss_ss: 2.54, loss_tt: 8.35, loss_adv: 0.03, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.23, D_fake: 0.27\n",
            "e: 101\n",
            "pre_loss: 10.53, loss_ss: 1.77, loss_tt: 8.46, loss_adv: 0.20, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.54, D_real: 0.42, D_fake: 0.12\n",
            "e: 102\n",
            "pre_loss: 10.75, loss_ss: 2.29, loss_tt: 8.32, loss_adv: 0.04, content: 0.06, style: 0.02\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 103\n",
            "pre_loss: 10.66, loss_ss: 2.62, loss_tt: 7.89, loss_adv: 0.05, content: 0.06, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 104\n",
            "pre_loss: 10.10, loss_ss: 1.46, loss_tt: 8.50, loss_adv: 0.05, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.20, D_fake: 0.30\n",
            "e: 105\n",
            "pre_loss: 10.24, loss_ss: 1.83, loss_tt: 8.25, loss_adv: 0.07, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 106\n",
            "pre_loss: 10.54, loss_ss: 2.44, loss_tt: 7.94, loss_adv: 0.06, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 107\n",
            "pre_loss: 10.84, loss_ss: 2.43, loss_tt: 8.22, loss_adv: 0.09, content: 0.06, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 108\n",
            "pre_loss: 10.43, loss_ss: 2.06, loss_tt: 8.19, loss_adv: 0.08, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 109\n",
            "pre_loss: 10.26, loss_ss: 1.71, loss_tt: 8.37, loss_adv: 0.08, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 110\n",
            "pre_loss: 11.11, loss_ss: 2.33, loss_tt: 8.61, loss_adv: 0.09, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 111\n",
            "pre_loss: 10.01, loss_ss: 1.96, loss_tt: 7.79, loss_adv: 0.17, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.51, D_real: 0.32, D_fake: 0.19\n",
            "e: 112\n",
            "pre_loss: 10.35, loss_ss: 2.11, loss_tt: 8.06, loss_adv: 0.07, content: 0.07, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 113\n",
            "pre_loss: 10.02, loss_ss: 1.71, loss_tt: 8.16, loss_adv: 0.05, content: 0.06, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 114\n",
            "pre_loss: 10.40, loss_ss: 2.31, loss_tt: 7.83, loss_adv: 0.16, content: 0.06, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 115\n",
            "pre_loss: 10.36, loss_ss: 1.78, loss_tt: 8.40, loss_adv: 0.07, content: 0.06, style: 0.02\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 116\n",
            "pre_loss: 10.20, loss_ss: 1.93, loss_tt: 8.13, loss_adv: 0.04, content: 0.06, style: 0.02\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 117\n",
            "pre_loss: 10.84, loss_ss: 2.20, loss_tt: 8.44, loss_adv: 0.09, content: 0.06, style: 0.02\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 118\n",
            "pre_loss: 10.04, loss_ss: 1.37, loss_tt: 8.46, loss_adv: 0.10, content: 0.07, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 119\n",
            "pre_loss: 10.55, loss_ss: 1.85, loss_tt: 8.55, loss_adv: 0.05, content: 0.06, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.27, D_fake: 0.23\n",
            "e: 120\n",
            "pre_loss: 11.21, loss_ss: 2.48, loss_tt: 8.58, loss_adv: 0.06, content: 0.06, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 121\n",
            "pre_loss: 10.43, loss_ss: 2.28, loss_tt: 7.99, loss_adv: 0.05, content: 0.06, style: 0.02\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 122\n",
            "pre_loss: 10.85, loss_ss: 2.04, loss_tt: 8.62, loss_adv: 0.09, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 123\n",
            "pre_loss: 10.28, loss_ss: 1.74, loss_tt: 8.36, loss_adv: 0.07, content: 0.06, style: 0.02\n",
            "discriminator_loss: 0.50, D_real: 0.23, D_fake: 0.27\n",
            "e: 124\n",
            "pre_loss: 11.20, loss_ss: 2.42, loss_tt: 8.57, loss_adv: 0.11, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 125\n",
            "pre_loss: 10.64, loss_ss: 2.38, loss_tt: 8.03, loss_adv: 0.14, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 126\n",
            "pre_loss: 10.39, loss_ss: 1.74, loss_tt: 8.44, loss_adv: 0.12, content: 0.05, style: 0.02\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 127\n",
            "pre_loss: 9.94, loss_ss: 1.68, loss_tt: 8.04, loss_adv: 0.14, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 128\n",
            "pre_loss: 10.44, loss_ss: 2.17, loss_tt: 8.01, loss_adv: 0.18, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.28, D_fake: 0.22\n",
            "e: 129\n",
            "pre_loss: 10.17, loss_ss: 1.73, loss_tt: 8.19, loss_adv: 0.16, content: 0.05, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 130\n",
            "pre_loss: 10.39, loss_ss: 2.29, loss_tt: 7.86, loss_adv: 0.16, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 131\n",
            "pre_loss: 10.63, loss_ss: 2.24, loss_tt: 8.13, loss_adv: 0.18, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 132\n",
            "pre_loss: 10.50, loss_ss: 1.73, loss_tt: 8.57, loss_adv: 0.14, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.22, D_fake: 0.29\n",
            "e: 133\n",
            "pre_loss: 10.50, loss_ss: 2.08, loss_tt: 8.16, loss_adv: 0.19, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 134\n",
            "pre_loss: 10.14, loss_ss: 2.05, loss_tt: 7.84, loss_adv: 0.18, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 135\n",
            "pre_loss: 11.26, loss_ss: 2.20, loss_tt: 8.80, loss_adv: 0.18, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 136\n",
            "pre_loss: 10.79, loss_ss: 2.31, loss_tt: 8.23, loss_adv: 0.18, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 137\n",
            "pre_loss: 10.88, loss_ss: 2.69, loss_tt: 7.96, loss_adv: 0.16, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.23, D_fake: 0.27\n",
            "e: 138\n",
            "pre_loss: 10.71, loss_ss: 2.01, loss_tt: 8.45, loss_adv: 0.17, content: 0.04, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.24, D_fake: 0.26\n",
            "e: 139\n",
            "pre_loss: 10.08, loss_ss: 1.54, loss_tt: 8.29, loss_adv: 0.19, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 140\n",
            "pre_loss: 10.01, loss_ss: 1.34, loss_tt: 8.42, loss_adv: 0.19, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 141\n",
            "pre_loss: 10.16, loss_ss: 1.43, loss_tt: 8.47, loss_adv: 0.20, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 142\n",
            "pre_loss: 10.19, loss_ss: 1.49, loss_tt: 8.43, loss_adv: 0.20, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.26, D_fake: 0.24\n",
            "e: 143\n",
            "pre_loss: 10.98, loss_ss: 1.87, loss_tt: 8.85, loss_adv: 0.20, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 144\n",
            "pre_loss: 10.05, loss_ss: 1.70, loss_tt: 8.08, loss_adv: 0.20, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 145\n",
            "pre_loss: 11.69, loss_ss: 2.69, loss_tt: 8.74, loss_adv: 0.20, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 146\n",
            "pre_loss: 9.98, loss_ss: 1.40, loss_tt: 8.32, loss_adv: 0.20, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 147\n",
            "pre_loss: 10.29, loss_ss: 1.88, loss_tt: 8.14, loss_adv: 0.20, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 148\n",
            "pre_loss: 10.61, loss_ss: 2.15, loss_tt: 8.19, loss_adv: 0.21, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n",
            "e: 149\n",
            "pre_loss: 10.02, loss_ss: 1.87, loss_tt: 7.88, loss_adv: 0.21, content: 0.03, style: 0.01\n",
            "discriminator_loss: 0.50, D_real: 0.25, D_fake: 0.25\n"
          ]
        }
      ],
      "source": [
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "\n",
        "real_label = Variable(Tensor(batchSize).fill_(1.0), requires_grad=False)\n",
        "fake_label = Variable(Tensor(batchSize).fill_(0.0), requires_grad=False)\n",
        "real_label = real_label[:, None]\n",
        "fake_label = fake_label[:, None]\n",
        "\n",
        "\n",
        "input_A = Tensor(batchSize, 3, image_size, image_size)\n",
        "input_B = Tensor(batchSize, 3, image_size, image_size)\n",
        "\n",
        "for epoch in range(0, 200):\n",
        "    i = -1\n",
        "    for batchA, batchB in zip(target_train, source_train):\n",
        "        i+= 1\n",
        "        real_A = Variable(input_A.copy_(batchA[0]))\n",
        "        real_B = Variable(input_B.copy_(batchB[0]))\n",
        "\n",
        "        optimizer_pre.zero_grad()\n",
        "        mixed_image, reconstructionA, reconstructionB = perceptual(real_A, real_B)\n",
        "\n",
        "        loss_ss = criterion_construct(reconstructionA, real_A) * 15.0\n",
        "        loss_tt = criterion_construct(reconstructionB, real_B) * 15.0\n",
        "\n",
        "        TV_loss = total_variation_loss(mixed_image)\n",
        "        pred_fake = discriminator(mixed_image)\n",
        "        loss_adv = criterion_adv(pred_fake, real_label)\n",
        "\n",
        "        cuda_mixed_image = mixed_image.clone().requires_grad_(True).cuda()\n",
        "        cuda_real_A = real_A.clone().requires_grad_(True).cuda()\n",
        "        cuda_real_B = real_B.clone().requires_grad_(True).cuda()\n",
        "        style_features = get_features(cuda_real_A, vgg)\n",
        "        content_features = get_features(cuda_real_B, vgg)\n",
        "        target_features = get_features(cuda_mixed_image, vgg)\n",
        "\n",
        "        content_loss = compute_content_loss(target_features['conv4_2'], content_features['conv4_2']) * 0.1\n",
        "        style_loss = compute_style_loss(style_features, target_features) * 0.05\n",
        "\n",
        "        preceptual_loss = loss_ss + loss_tt + TV_loss + loss_adv + content_loss + style_loss\n",
        "\n",
        "        preceptual_loss.backward()\n",
        "        optimizer_pre.step()\n",
        "\n",
        "        optimizer_dis.zero_grad()\n",
        "\n",
        "        pred_real = discriminator(real_A)\n",
        "        pred_fake = discriminator(mixed_image.detach())\n",
        "\n",
        "        loss_dis_real = criterion_discriminator(pred_real, real_label)\n",
        "        loss_dis_fake = criterion_discriminator(pred_fake, fake_label)\n",
        "\n",
        "        discriminator_loss = loss_dis_real + loss_dis_fake\n",
        "        discriminator_loss.backward()\n",
        "        optimizer_dis.step()\n",
        "\n",
        "        if  i % 200 == 0:\n",
        "            real_A = real_A.data\n",
        "            real_B = real_B.data\n",
        "            mixed_image = mixed_image.data\n",
        "            reconstructionA = reconstructionA.data\n",
        "            reconstructionB = reconstructionB.data\n",
        "\n",
        "            save_image(real_A, './output/%03d/%d_A.png' % ( epoch, i))\n",
        "            save_image(real_B, 'output/%03d/%d_B.png' % ( epoch, i))\n",
        "            save_image(reconstructionA, 'output/%03d/%d_reconA.png' % ( epoch, i))\n",
        "            save_image(reconstructionB, 'output/%03d/%d_reconB.png' % ( epoch, i))\n",
        "            save_image(mixed_image, 'output/%03d/%d_Mixed.png' % ( epoch, i))\n",
        "\n",
        "    print (\"e: %d\" % epoch)    \n",
        "    print (\"pre_loss: %.2f, loss_ss: %.2f, loss_tt: %.2f, loss_adv: %.2f, content: %.2f, style: %.2f\" % (preceptual_loss, loss_ss, loss_tt, loss_adv, content_loss, style_loss))\n",
        "    print (\"discriminator_loss: %.2f, D_real: %.2f, D_fake: %.2f\" % (discriminator_loss, loss_dis_real, loss_dis_fake))\n",
        "\n",
        "    scheduler_pre.step()\n",
        "    scheduler_dis.step()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAcUzkPxdbsO"
      },
      "outputs": [],
      "source": [
        "torch.save(perceptual, \"./model_weight/perceptual\")\n",
        "torch.save(discriminator, \"./model_weight/discriminator\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resulting image"
      ],
      "metadata": {
        "id": "3mtbjZVKba5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = imread('./output/199/00_400_Mixed.png')\n",
        "\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "5zOTBBi04khW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = imread('./output/199/00_400_reconB.png')\n",
        "\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "qKxxfZPF4pBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "my_model.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0abf52f7dff1bbf2191b90c10bb43e97e891f8d70dafe2d0c71717742c591866"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f7f15cde5f634963a0b99c5e56853187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9de528844184af391607c2a2b0bdf34",
              "IPY_MODEL_a804f7f455ba4ed1b3f434584d639c63",
              "IPY_MODEL_9acab382674d4a49a17ea878306cba5c"
            ],
            "layout": "IPY_MODEL_44cb3e79acc54c9fb062364f3c73efad"
          }
        },
        "f9de528844184af391607c2a2b0bdf34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8209b4880d7e4b32a44d985573648030",
            "placeholder": "​",
            "style": "IPY_MODEL_aae6a9c29efd4d3aa23d7588d381cb90",
            "value": "100%"
          }
        },
        "a804f7f455ba4ed1b3f434584d639c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35d513648d294b78b3eb05ab16c4a2ce",
            "max": 531460341,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_953487c6c75b4fba848b9f16586ac16b",
            "value": 531460341
          }
        },
        "9acab382674d4a49a17ea878306cba5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f94d0dc647d4403afd35351cf28b37c",
            "placeholder": "​",
            "style": "IPY_MODEL_9a77e91128f44650929dacd97cd72576",
            "value": " 507M/507M [00:04&lt;00:00, 161MB/s]"
          }
        },
        "44cb3e79acc54c9fb062364f3c73efad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8209b4880d7e4b32a44d985573648030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aae6a9c29efd4d3aa23d7588d381cb90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35d513648d294b78b3eb05ab16c4a2ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953487c6c75b4fba848b9f16586ac16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f94d0dc647d4403afd35351cf28b37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a77e91128f44650929dacd97cd72576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}